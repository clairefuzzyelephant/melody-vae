{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import PIL\n",
    "\n",
    "import torchvision as tv\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "\n",
    "from pushover import notify\n",
    "from utils import makegif\n",
    "from random import randint\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32 # batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1339, 42)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "\n",
    "def load_img(img_path):\n",
    "    img = PIL.Image.open(img_path)\n",
    "#     img = PIL.ImageMath.eval(\"int(img)\", img=img)\n",
    "#     print(img)\n",
    "#     img = PIL.ImageOps.grayscale(img)\n",
    "    img = img.convert(mode=\"L\")\n",
    "#     print(img)\n",
    "    return img\n",
    "IMRANGE = 256 # uint8\n",
    "dataset = datasets.ImageFolder(root='trainings/rolls_gray', transform=transforms.Compose([\n",
    "#     transforms.Resize(64),\n",
    "    transforms.ToTensor(), \n",
    "#     lambda x: (x * IMRANGE).type(torch.IntTensor),\n",
    "#     lambda x: x.type(torch.FloatTensor),\n",
    "\n",
    "]), loader=load_img)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "len(dataset.imgs), len(dataloader)\n",
    "\n",
    "# size of input = 3 x 128 x 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed input for debugging\n",
    "fixed_x, _ = next(iter(dataloader))\n",
    "save_image(fixed_x, 'outputs/real_image.png')\n",
    "\n",
    "# Image('outputs/real_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][0].shape)\n",
    "sample_dat = dataset[0][0]\n",
    "# print(dataset[0])\n",
    "# for i in range(128):\n",
    "#     for j in range(128):\n",
    "#         if sample_dat[0][i][j] > 0:\n",
    "#             print(i, j, sample_dat[0][i][j])\n",
    "HSIZE = 2048 #9216 # 1024\n",
    "ZDIM =  32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "#         print(\"flatten: \", input.shape)\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=HSIZE):\n",
    "        return input.view(input.size(0), size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=1, h_dim=HSIZE, z_dim=ZDIM):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2), # -> [32, 32, 31, 31] 63\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # -> [32, 64, 14, 14] 31\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2), # -> [32, 128, 6, 6] 14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2), # -> [32, 256, 2, 2] 6\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2), # -> Null -> [32, 512, 2, 2] \n",
    "            nn.ReLU(), \n",
    "            Flatten() # -> [32, 1024]  -> [32, 2048]\n",
    "            # [32, a, b, c] -> [32, abc]\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(), \n",
    "            nn.ConvTranspose2d(h_dim, 256 , kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             UnFlatten(),\n",
    "#             nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "#             nn.Sigmoid(),\n",
    "#         )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "#         print(\"bottle: \",mu.shape, logvar.shape)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "#         print(\"======== Encode ========\", x.shape)\n",
    "        h = self.encoder(x)\n",
    "#         print(\"enc(x): \", h.shape)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "#         print(\"z.shape: \", z.shape)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "#         print(\"======== Decode ========\", z.shape)\n",
    "        z = self.fc3(z)\n",
    "#         print(\"fc3(z).shape: \", z.shape)\n",
    "        z = self.decoder(z)\n",
    "#         print(\"decode(fc3(z)).shape: \", z.shape)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "#         print(z.shape)\n",
    "        z = self.decode(z)\n",
    "#         print(z.shape, mu.shape, logvar.shape)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channels = fixed_x.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(image_channels=image_channels).to(device)\n",
    "# model.load_state_dict(torch.load('vae.torch', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "#     BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD, BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/50] Loss: 3128.845 3124.562 4.283 tensor(100123.0547) tensor(137.0697) 32\n",
      "Epoch[2/50] Loss: 318.860 318.489 0.370 tensor(10203.5068) tensor(11.8442) 32\n",
      "Epoch[3/50] Loss: 296.285 295.983 0.302 tensor(9481.1357) tensor(9.6645) 32\n",
      "Epoch[4/50] Loss: 331.195 330.912 0.283 tensor(10598.2295) tensor(9.0459) 32\n",
      "Epoch[5/50] Loss: 322.957 322.722 0.236 tensor(10334.6357) tensor(7.5385) 32\n",
      "Epoch[6/50] Loss: 297.832 297.580 0.252 tensor(9530.6123) tensor(8.0516) 32\n",
      "Epoch[7/50] Loss: 325.332 325.095 0.237 tensor(10410.6104) tensor(7.5681) 32\n",
      "Epoch[8/50] Loss: 307.353 307.099 0.255 tensor(9835.3037) tensor(8.1482) 32\n",
      "Epoch[9/50] Loss: 300.357 300.127 0.231 tensor(9611.4395) tensor(7.3771) 32\n",
      "Epoch[10/50] Loss: 337.542 337.310 0.232 tensor(10801.3438) tensor(7.4304) 32\n",
      "Epoch[11/50] Loss: 326.421 326.212 0.209 tensor(10445.4805) tensor(6.6891) 32\n",
      "Epoch[12/50] Loss: 296.958 296.715 0.243 tensor(9502.6475) tensor(7.7656) 32\n",
      "Epoch[13/50] Loss: 307.886 307.672 0.214 tensor(9852.3457) tensor(6.8360) 32\n",
      "Epoch[14/50] Loss: 257.294 257.059 0.235 tensor(8233.4199) tensor(7.5313) 32\n",
      "Epoch[15/50] Loss: 271.094 270.877 0.217 tensor(8674.9922) tensor(6.9287) 32\n",
      "Epoch[16/50] Loss: 293.271 293.061 0.211 tensor(9384.6865) tensor(6.7504) 32\n",
      "Epoch[17/50] Loss: 266.379 266.150 0.229 tensor(8524.1328) tensor(7.3367) 32\n",
      "Epoch[18/50] Loss: 294.845 294.611 0.235 tensor(9435.0498) tensor(7.5067) 32\n",
      "Epoch[19/50] Loss: 253.102 252.887 0.214 tensor(8099.2559) tensor(6.8560) 32\n",
      "Epoch[20/50] Loss: 282.946 282.735 0.211 tensor(9054.2686) tensor(6.7631) 32\n",
      "Epoch[21/50] Loss: 249.389 249.181 0.208 tensor(7980.4502) tensor(6.6447) 32\n",
      "Epoch[22/50] Loss: 293.926 293.723 0.202 tensor(9405.6182) tensor(6.4728) 32\n",
      "Epoch[23/50] Loss: 259.669 259.461 0.208 tensor(8309.4023) tensor(6.6638) 32\n",
      "Epoch[24/50] Loss: 300.553 300.327 0.225 tensor(9617.6865) tensor(7.2126) 32\n",
      "Epoch[25/50] Loss: 289.782 289.592 0.190 tensor(9273.0088) tensor(6.0700) 32\n",
      "Epoch[26/50] Loss: 245.398 245.151 0.247 tensor(7852.7456) tensor(7.9200) 32\n",
      "Epoch[27/50] Loss: 234.643 234.393 0.250 tensor(7508.5654) tensor(7.9998) 32\n",
      "Epoch[28/50] Loss: 260.506 260.236 0.270 tensor(8336.1895) tensor(8.6520) 32\n",
      "Epoch[29/50] Loss: 254.404 254.161 0.243 tensor(8140.9307) tensor(7.7796) 32\n",
      "Epoch[30/50] Loss: 264.460 264.208 0.252 tensor(8462.7090) tensor(8.0663) 32\n",
      "Epoch[31/50] Loss: 261.089 260.843 0.247 tensor(8354.8584) tensor(7.8974) 32\n",
      "Epoch[32/50] Loss: 231.570 231.308 0.262 tensor(7410.2349) tensor(8.3790) 32\n",
      "Epoch[33/50] Loss: 252.284 252.017 0.267 tensor(8073.0757) tensor(8.5457) 32\n",
      "Epoch[34/50] Loss: 255.698 255.466 0.232 tensor(8182.3364) tensor(7.4135) 32\n",
      "Epoch[35/50] Loss: 246.056 245.781 0.275 tensor(7873.7856) tensor(8.7999) 32\n",
      "Epoch[36/50] Loss: 224.220 223.957 0.263 tensor(7175.0386) tensor(8.4282) 32\n",
      "Epoch[37/50] Loss: 246.552 246.307 0.245 tensor(7889.6680) tensor(7.8560) 32\n",
      "Epoch[38/50] Loss: 231.173 230.915 0.257 tensor(7397.5225) tensor(8.2291) 32\n",
      "Epoch[39/50] Loss: 225.884 225.637 0.247 tensor(7228.2798) tensor(7.9095) 32\n",
      "Epoch[40/50] Loss: 224.665 224.403 0.262 tensor(7189.2656) tensor(8.3803) 32\n",
      "Epoch[41/50] Loss: 223.151 222.881 0.270 tensor(7140.8477) tensor(8.6433) 32\n",
      "Epoch[42/50] Loss: 242.614 242.370 0.245 tensor(7763.6636) tensor(7.8316) 32\n",
      "Epoch[43/50] Loss: 246.037 245.790 0.246 tensor(7873.1689) tensor(7.8793) 32\n",
      "Epoch[44/50] Loss: 235.120 234.872 0.249 tensor(7523.8501) tensor(7.9532) 32\n",
      "Epoch[45/50] Loss: 235.312 235.083 0.229 tensor(7529.9844) tensor(7.3277) 32\n",
      "Epoch[46/50] Loss: 254.020 253.804 0.216 tensor(8128.6377) tensor(6.9224) 32\n",
      "Epoch[47/50] Loss: 242.511 242.275 0.236 tensor(7760.3618) tensor(7.5648) 32\n",
      "Epoch[48/50] Loss: 223.320 223.066 0.254 tensor(7146.2295) tensor(8.1184) 32\n",
      "Epoch[49/50] Loss: 219.210 218.926 0.284 tensor(7014.7163) tensor(9.0866) 32\n",
      "Epoch[50/50] Loss: 217.728 217.462 0.266 tensor(6967.2876) tensor(8.5104) 32\n",
      "Notifing Epoch[50/50] Loss: 217.728 217.462 0.266 Just because I have to put something. <Response [400]>\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for idx, (images, _) in enumerate(dataloader):\n",
    "        recon_images, mu, logvar = vae(images)\n",
    "#         print(images.shape)\n",
    "        loss, bce, kld = loss_fn(recon_images, images, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#         to_print = \"Epoch[{}/{}] Loss: {:.3f} {:.3f} {:.3f}\".format(epoch+1, \n",
    "#                                 epochs, loss.data[0]/bs, bce.data[0]/bs, kld.data[0]/bs)\n",
    "        to_print = \"Epoch[{}/{}] Loss: {:.3f} {:.3f} {:.3f}\".format(epoch+1, \n",
    "                                epochs, loss.data/bs, bce.data/bs, kld.data/bs)\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        torch.save(vae.state_dict(), 'models/vae.torch-grayv-nimgs_{}-epochs_{}-{}'.format(len(dataset.imgs), epoch, epochs))\n",
    "        \n",
    "    print(to_print, loss.data, kld.data, bs)\n",
    "\n",
    "# notify to android when finished training\n",
    "notify(to_print, priority=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'models/vae.torch-grayv-nimgs_{}-epochs_{}'.format(len(dataset.imgs), epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
