{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "\n",
    "from pushover import notify\n",
    "from utils import makegif\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "import IPython\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "import PIL\n",
    "\n",
    "import piano_roll_utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32 # batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "dataset = datasets.ImageFolder(root='trainings/roll_rgb_test', transform=transforms.Compose([\n",
    "#     transforms.Resize(64),\n",
    "    transforms.ToTensor(), \n",
    "]))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "len(dataset.imgs), len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAA80lEQVR4nO3aobGDUBAF0Pc6oAckBeAjkRRAcRSAROIQFIBMD5Fx+SIOftxOyMA5cndmd82dJyAlAAAAAAAAAAAAAAAAAAAAAAAAAAAAAH5FPnD3MAyB09q2DZzGVeSUUt/3+0bXdZvKNE2xu2+3W+C02Dy9SdX55WVZ6rreN/ax2Gfi9AJD/ynuRz7ClzLP89EnAAAAAAAAwAGO/CR5v98Dp5VlGTiNq8gppXVd942qqjaVx+MRu7soisBpsXl6k6rzy6/XK+d/XoJxHDeVpmm+ctIPCQz9p7j7L+hLns/n0ScAAAAAAAAAAAAAAAAAAAT6A73KL0A6uTVxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixed input for debugging\n",
    "fixed_x, _ = next(iter(dataloader))\n",
    "\n",
    "\n",
    "save_image(fixed_x, 'tmp/real_image.png')\n",
    "\n",
    "Image('tmp/real_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][0].shape)\n",
    "HSIZE = 2048 #9216 # 1024\n",
    "ZDIM =  32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gt_zero_elem(matrix):\n",
    "    print(matrix[matrix > 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=HSIZE):\n",
    "        return input.view(input.size(0), size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=HSIZE, z_dim=ZDIM):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2), # -> [32, 32, 31, 31] 63\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # -> [32, 64, 14, 14] 31\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2), # -> [32, 128, 6, 6] 14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2), # -> [32, 256, 2, 2] 6\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2), # -> Null -> [32, 512, 2, 2] \n",
    "            nn.ReLU(), \n",
    "            Flatten() # -> [32, 1024]  -> [32, 2048]\n",
    "            # [32, a, b, c] -> [32, abc]\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(), \n",
    "            nn.ConvTranspose2d(h_dim, 256 , kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             UnFlatten(),\n",
    "#             nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "#             nn.Sigmoid(),\n",
    "#         )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "#         print(\"bottle: \",mu.shape, logvar.shape)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "#         print(\"======== Encode ========\", x.shape)\n",
    "        h = self.encoder(x)\n",
    "#         print(\"enc(x): \", h.shape)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "#         print(\"z.shape: \", z.shape)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "#         print(\"======== Decode ========\", z.shape)\n",
    "        z = self.fc3(z)\n",
    "#         print(\"fc3(z).shape: \", z.shape)\n",
    "        z = self.decoder(z)\n",
    "#         print(\"decode(fc3(z)).shape: \", z.shape)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "#         print(z.shape)\n",
    "        z = self.decode(z)\n",
    "#         print(z.shape, mu.shape, logvar.shape)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channels = fixed_x.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(image_channels=image_channels).to(device)\n",
    "model_version = \"velo-alb-nimgs_1339-epochs_50\" #\"alb-nimgs_4312-epochs_50\" #\"AC-nimgs_2515-epochs_50\" vae.torch-alb-nimgs_4312-epochs_50\n",
    "model.load_state_dict(torch.load('models/vae.torch-' + model_version, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(x):\n",
    "    recon_x, _, _ = model(x)\n",
    "    return torch.cat([x, recon_x])\n",
    "\n",
    "def get_compare_img(x, recon):\n",
    "    return torch.cat([x, recon])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_x = dataset[0][0].unsqueeze(0)\n",
    "fixed_x2 = fixed_x.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAA80lEQVR4nO3aobGDUBAF0Pc6oAckBeAjkRRAcRSAROIQFIBMD5Fx+SIOftxOyMA5cndmd82dJyAlAAAAAAAAAAAAAAAAAAAAAAAAAAAAAH5FPnD3MAyB09q2DZzGVeSUUt/3+0bXdZvKNE2xu2+3W+C02Dy9SdX55WVZ6rreN/ax2Gfi9AJD/ynuRz7ClzLP89EnAAAAAAAAwAGO/CR5v98Dp5VlGTiNq8gppXVd942qqjaVx+MRu7soisBpsXl6k6rzy6/XK+d/XoJxHDeVpmm+ctIPCQz9p7j7L+hLns/n0ScAAAAAAAAAAAAAAAAAAAT6A73KL0A6uTVxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "unconfined": true,
       "width": 300
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rgb_f = 'outputs/rgb-{}.png'.format(model_version)\n",
    "save_image(fixed_x.data.cpu(), rgb_f)\n",
    "display(Image(rgb_f, width=300, unconfined=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58.         58.         58.         58.         58.         58.\n",
      " 58.         58.         58.         58.         58.         58.\n",
      " 58.         58.         58.         58.         58.         58.\n",
      " 58.         58.         58.         58.         58.         58.\n",
      " 58.         58.         54.         54.         54.         54.\n",
      " 54.         54.         54.         63.33333333 63.33333333 63.33333333\n",
      " 63.33333333 63.33333333 63.33333333 63.33333333 63.33333333 63.33333333\n",
      " 63.33333333 63.33333333 63.33333333 63.33333333 63.33333333 63.33333333\n",
      " 63.33333333 63.33333333 63.33333333 63.33333333 63.33333333 63.33333333\n",
      " 63.33333333 63.33333333 63.33333333 63.33333333 63.33333333 63.33333333\n",
      " 58.         58.         58.         58.         58.         58.\n",
      " 58.         58.         58.         58.         58.         58.\n",
      " 58.         58.         58.         58.         58.         58.\n",
      " 58.         58.         58.         58.         58.         58.\n",
      " 58.         58.         58.         58.         67.         54.\n",
      " 54.         54.         54.         54.         54.         63.33333333\n",
      " 63.33333333 63.33333333 63.33333333 63.33333333 63.33333333 63.33333333\n",
      " 63.33333333 63.33333333 63.33333333 63.33333333 63.33333333 63.33333333\n",
      " 63.33333333 63.33333333 63.33333333 63.33333333 63.33333333 63.33333333\n",
      " 63.33333333 63.33333333 63.33333333 63.33333333 63.33333333 63.33333333\n",
      " 65.66666667 65.66666667 65.66666667 65.66666667 65.66666667 65.66666667\n",
      " 65.66666667 65.66666667 73.66666667 73.66666667 73.66666667 73.66666667\n",
      " 73.66666667 73.66666667 73.66666667 73.66666667 73.66666667 73.66666667\n",
      " 73.66666667 73.66666667 73.66666667 73.66666667 73.66666667 73.66666667\n",
      " 73.66666667 73.66666667 73.66666667 73.66666667 73.66666667 73.66666667\n",
      " 73.66666667 73.66666667 73.66666667 73.66666667 71.         71.\n",
      " 71.         71.         71.         71.         71.         80.\n",
      " 80.         80.         80.         80.         80.         80.\n",
      " 80.         80.         80.         80.         80.         80.\n",
      " 80.         80.         80.         80.         80.         80.\n",
      " 80.         80.         80.         80.         80.         80.\n",
      " 80.         80.         73.66666667 73.66666667 73.66666667 73.66666667\n",
      " 73.66666667 73.66666667 73.66666667 73.66666667 73.66666667 73.66666667\n",
      " 73.66666667 73.66666667 73.66666667 73.66666667 73.66666667 73.66666667\n",
      " 73.66666667 73.66666667 73.66666667 73.66666667 73.66666667 73.66666667\n",
      " 73.66666667 73.66666667 73.66666667 73.66666667 73.66666667 73.66666667\n",
      " 85.         60.66666667 60.66666667 60.66666667 60.66666667 60.66666667\n",
      " 60.66666667 80.         80.         80.         80.         80.\n",
      " 80.         80.         80.         80.         80.         80.\n",
      " 80.         80.         80.         80.         80.         80.\n",
      " 80.         80.         80.         80.         80.         80.\n",
      " 80.         80.         82.66666667 82.66666667 82.66666667 82.66666667\n",
      " 82.66666667 82.66666667 82.66666667 82.66666667]\n"
     ]
    }
   ],
   "source": [
    "ori = PIL.Image.open(rgb_f)\n",
    "ori = ori.convert(mode=\"L\")\n",
    "roll = np.array(ori)/3\n",
    "\n",
    "from skimage import color\n",
    "from skimage import io\n",
    "\n",
    "img = color.rgb2gray(io.imread(rgb_f))\n",
    "\n",
    "midi= piano_roll_utils.piano_roll_to_pretty_midi(roll,fs=50,program=11)\n",
    "IPython.display.Audio(midi.fluidsynth(fs=44100),rate=44100)\n",
    "print(roll[roll>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_x = compare(fixed_x)\n",
    "recon, _, _ = model(fixed_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_f = 'outputs/recon-{}.png'.format(model_version)\n",
    "save_image(recon.data.cpu(), recon_f)\n",
    "display(Image(recon_f, width=300, unconfined=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon = PIL.Image.open(recon_f)\n",
    "recon = recon.convert(mode=\"L\")\n",
    "roll_res = np.array(recon)/3\n",
    "filt = roll_res > 10\n",
    "roll_res = roll_res * filt\n",
    "\n",
    "filt = roll_res < 70\n",
    "roll_res = roll_res * filt\n",
    "# print(roll_res.shape)\n",
    "print(roll_res[roll_res>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi= piano_roll_utils.piano_roll_to_pretty_midi(roll_res,fs=50,program=11)\n",
    "IPython.display.Audio(midi.fluidsynth(fs=44100),rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128)\n"
     ]
    }
   ],
   "source": [
    "img = PIL.Image.open(\"trainings/roll_gray_test/alb_esp1_format0/st-256.png\")\n",
    "img = img.convert(mode=\"L\")\n",
    "roll_res = np.array(img)\n",
    "print(roll_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45\n",
      " 45 45 42 42 42 42 42 42 42 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49\n",
      " 49 49 49 49 49 49 49 49 49 49 49 49 45 45 45 45 45 45 45 45 45 45 45 45\n",
      " 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 52 42 42 42 42 42 42 49\n",
      " 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49\n",
      " 51 51 51 51 51 51 51 51 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57\n",
      " 57 57 57 57 57 57 57 57 57 57 55 55 55 55 55 55 55 62 62 62 62 62 62 62\n",
      " 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 57 57 57 57\n",
      " 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57\n",
      " 66 47 47 47 47 47 47 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62\n",
      " 62 62 62 62 62 62 62 62 64 64 64 64 64 64 64 64]\n"
     ]
    }
   ],
   "source": [
    "midi= piano_roll_utils.piano_roll_to_pretty_midi(roll_res,fs=50,program=11)\n",
    "IPython.display.Audio(midi.fluidsynth(fs=44100),rate=44100)\n",
    "print(roll_res[roll_res>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = roll_res \n",
    "midi= piano_roll_utils.piano_roll_to_pretty_midi(roll_res,fs=50,program=11)\n",
    "IPython.display.Audio(midi.fluidsynth(fs=44100),rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(fixed_x2, 'tmp/test.png', padding=0)\n",
    "display(Image('tmp/test.png'))\n",
    "z, mu, log_var = model.encode(fixed_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
