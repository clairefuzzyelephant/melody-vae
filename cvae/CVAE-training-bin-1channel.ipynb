{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import PIL\n",
    "\n",
    "import torchvision as tv\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "\n",
    "from pushover import notify\n",
    "from utils import makegif\n",
    "from random import randint\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1 # batchsize 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1339, 1339)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "\n",
    "def print_gt_zero_elem(matrix):\n",
    "    print(matrix[matrix > 0])\n",
    "    \n",
    "# def load_img(img_path):\n",
    "#     img = PIL.Image.open(img_path)\n",
    "# #     img = PIL.ImageMath.eval(\"int(img)\", img=img)\n",
    "# #     print(img)\n",
    "# #     img = PIL.ImageOps.grayscale(img)\n",
    "#     img = img.convert(mode=\"L\")\n",
    "#     npimg = np.array(img)\n",
    "#     npimg = npimg/256.0\n",
    "#     return npimg\n",
    "# IMRANGE = 256 # uint8\n",
    "# # dataset = datasets.ImageFolder(root='trainings/rolls_gray', transform=transforms.Compose([\n",
    "# # #     transforms.Resize(64),\n",
    "# #     transforms.ToTensor(), \n",
    "# # #     lambda x: (x * IMRANGE).type(torch.IntTensor),\n",
    "# #     lambda x: x.type(torch.FloatTensor),\n",
    "# # ]), loader=load_img)\n",
    "\n",
    "# dataset = datasets.ImageFolder(root='trainings/rolls_gray', transform=transforms.Compose([\n",
    "#     transforms.ToTensor(), \n",
    "#     lambda x: x.type(torch.FloatTensor),\n",
    "# ]), loader=load_img)\n",
    "\n",
    "def load_img(img_path):\n",
    "    img = PIL.Image.open(img_path)\n",
    "    img = img.convert(mode=\"L\")\n",
    "#     npimg = np.array(img)/256.0\n",
    "    return img\n",
    "\n",
    "IMRANGE = 256 # uint8\n",
    "\n",
    "dataset = datasets.ImageFolder(root='trainings/rolls_gray', transform=transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    lambda x: x > 0,\n",
    "    lambda x: x.type(torch.FloatTensor),\n",
    "]), loader=load_img)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "len(dataset.imgs), len(dataloader)\n",
    "\n",
    "# size of input = 3 x 128 x 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed input for debugging\n",
    "fixed_x, _ = next(iter(dataloader))\n",
    "save_image(fixed_x, 'outputs/real_image.png')\n",
    "\n",
    "# Image('outputs/real_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 128])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][0].shape)\n",
    "sample_dat = dataset[0][0]\n",
    "print_gt_zero_elem(sample_dat)\n",
    "HSIZE = 2048 #9216 # 1024\n",
    "ZDIM =  32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "#         print(\"flatten: \", input.shape)\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=HSIZE):\n",
    "        return input.view(input.size(0), size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=1, h_dim=HSIZE, z_dim=ZDIM):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2), # -> [32, 32, 31, 31] 63\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # -> [32, 64, 14, 14] 31\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2), # -> [32, 128, 6, 6] 14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2), # -> [32, 256, 2, 2] 6\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2), # -> Null -> [32, 512, 2, 2] \n",
    "            nn.ReLU(), \n",
    "            Flatten() # -> [32, 1024]  -> [32, 2048]\n",
    "            # [32, a, b, c] -> [32, abc]\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(), \n",
    "            nn.ConvTranspose2d(h_dim, 256 , kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             UnFlatten(),\n",
    "#             nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "#             nn.Sigmoid(),\n",
    "#         )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "#         print(\"bottle: \",mu.shape, logvar.shape)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "#         print(\"======== Encode ========\", x.shape)\n",
    "        h = self.encoder(x)\n",
    "#         print(\"enc(x): \", h.shape)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "#         print(\"z.shape: \", z.shape)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "#         print(\"======== Decode ========\", z.shape)\n",
    "        z = self.fc3(z)\n",
    "#         print(\"fc3(z).shape: \", z.shape)\n",
    "        z = self.decoder(z)\n",
    "#         print(\"decode(fc3(z)).shape: \", z.shape)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "#         print(z.shape)\n",
    "        z = self.decode(z)\n",
    "#         print(z.shape, mu.shape, logvar.shape)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channels = fixed_x.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(image_channels=image_channels).to(device)\n",
    "# model.load_state_dict(torch.load('vae.torch', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "#     BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD, BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"graybin_bce\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/50] Loss: 1323.707 1322.040 1.667 tensor(1323.7068) tensor(1.6667) 1\n",
      "Epoch[2/50] Loss: 1238.448 1237.332 1.116 tensor(1238.4482) tensor(1.1158) 1\n",
      "Epoch[3/50] Loss: 1155.286 1153.052 2.234 tensor(1155.2858) tensor(2.2342) 1\n",
      "Epoch[4/50] Loss: 1385.116 1383.161 1.955 tensor(1385.1163) tensor(1.9550) 1\n",
      "Epoch[5/50] Loss: 1606.427 1604.125 2.302 tensor(1606.4270) tensor(2.3019) 1\n",
      "Epoch[6/50] Loss: 696.221 693.394 2.827 tensor(696.2215) tensor(2.8272) 1\n",
      "Epoch[7/50] Loss: 1300.368 1298.059 2.308 tensor(1300.3676) tensor(2.3082) 1\n",
      "Epoch[8/50] Loss: 621.003 617.363 3.639 tensor(621.0027) tensor(3.6394) 1\n",
      "Epoch[9/50] Loss: 932.638 928.759 3.879 tensor(932.6377) tensor(3.8791) 1\n",
      "Epoch[10/50] Loss: 1272.932 1270.671 2.261 tensor(1272.9320) tensor(2.2612) 1\n",
      "Epoch[11/50] Loss: 506.531 503.439 3.092 tensor(506.5305) tensor(3.0916) 1\n",
      "Epoch[12/50] Loss: 764.249 760.731 3.518 tensor(764.2493) tensor(3.5184) 1\n",
      "Epoch[13/50] Loss: 746.649 743.370 3.280 tensor(746.6494) tensor(3.2797) 1\n",
      "Epoch[14/50] Loss: 852.725 849.800 2.925 tensor(852.7245) tensor(2.9247) 1\n",
      "Epoch[15/50] Loss: 26.956 23.881 3.074 tensor(26.9558) tensor(3.0745) 1\n",
      "Epoch[16/50] Loss: 1249.462 1246.455 3.007 tensor(1249.4624) tensor(3.0074) 1\n",
      "Epoch[17/50] Loss: 238.564 234.801 3.763 tensor(238.5639) tensor(3.7631) 1\n",
      "Epoch[18/50] Loss: 635.482 632.427 3.055 tensor(635.4818) tensor(3.0548) 1\n",
      "Epoch[19/50] Loss: 732.926 729.239 3.688 tensor(732.9264) tensor(3.6876) 1\n",
      "Epoch[20/50] Loss: 688.646 684.659 3.987 tensor(688.6463) tensor(3.9869) 1\n",
      "Epoch[21/50] Loss: 619.761 616.126 3.635 tensor(619.7611) tensor(3.6350) 1\n",
      "Epoch[22/50] Loss: 94.686 90.471 4.215 tensor(94.6856) tensor(4.2148) 1\n",
      "Epoch[23/50] Loss: 943.898 940.307 3.591 tensor(943.8983) tensor(3.5912) 1\n",
      "Epoch[24/50] Loss: 291.560 288.461 3.099 tensor(291.5598) tensor(3.0987) 1\n",
      "Epoch[25/50] Loss: 177.507 172.539 4.968 tensor(177.5071) tensor(4.9684) 1\n",
      "Epoch[26/50] Loss: 628.294 624.818 3.476 tensor(628.2936) tensor(3.4755) 1\n",
      "Epoch[27/50] Loss: 526.630 523.218 3.412 tensor(526.6296) tensor(3.4116) 1\n",
      "Epoch[28/50] Loss: 486.030 482.661 3.369 tensor(486.0302) tensor(3.3695) 1\n",
      "Epoch[29/50] Loss: 190.518 186.003 4.515 tensor(190.5177) tensor(4.5148) 1\n",
      "Epoch[30/50] Loss: 264.189 261.195 2.994 tensor(264.1891) tensor(2.9943) 1\n",
      "Epoch[31/50] Loss: 382.654 378.038 4.616 tensor(382.6541) tensor(4.6160) 1\n",
      "Epoch[32/50] Loss: 195.452 192.280 3.172 tensor(195.4518) tensor(3.1721) 1\n",
      "Epoch[33/50] Loss: 459.840 455.853 3.987 tensor(459.8400) tensor(3.9868) 1\n",
      "Epoch[34/50] Loss: 326.989 324.011 2.978 tensor(326.9892) tensor(2.9778) 1\n",
      "Epoch[35/50] Loss: 132.950 128.427 4.523 tensor(132.9502) tensor(4.5231) 1\n",
      "Epoch[36/50] Loss: 140.350 136.698 3.652 tensor(140.3501) tensor(3.6518) 1\n",
      "Epoch[37/50] Loss: 327.773 323.197 4.575 tensor(327.7726) tensor(4.5754) 1\n",
      "Epoch[38/50] Loss: 73.060 69.361 3.698 tensor(73.0599) tensor(3.6984) 1\n",
      "Epoch[39/50] Loss: 191.330 186.770 4.559 tensor(191.3298) tensor(4.5594) 1\n",
      "Epoch[40/50] Loss: 490.440 486.355 4.086 tensor(490.4403) tensor(4.0857) 1\n",
      "Epoch[41/50] Loss: 332.505 329.190 3.316 tensor(332.5053) tensor(3.3158) 1\n",
      "Epoch[42/50] Loss: 277.392 273.471 3.921 tensor(277.3920) tensor(3.9214) 1\n",
      "Epoch[43/50] Loss: 98.593 95.013 3.580 tensor(98.5932) tensor(3.5802) 1\n",
      "Epoch[44/50] Loss: 251.590 247.713 3.877 tensor(251.5903) tensor(3.8769) 1\n",
      "Epoch[45/50] Loss: 78.620 73.099 5.521 tensor(78.6201) tensor(5.5206) 1\n",
      "Epoch[46/50] Loss: 180.351 175.629 4.722 tensor(180.3507) tensor(4.7217) 1\n",
      "Epoch[47/50] Loss: 143.110 139.206 3.904 tensor(143.1101) tensor(3.9044) 1\n",
      "Epoch[48/50] Loss: 339.833 335.981 3.852 tensor(339.8329) tensor(3.8517) 1\n",
      "Epoch[49/50] Loss: 343.103 339.104 3.999 tensor(343.1026) tensor(3.9988) 1\n",
      "Epoch[50/50] Loss: 82.478 78.221 4.257 tensor(82.4782) tensor(4.2574) 1\n",
      "Notifing Epoch[50/50] Loss: 82.478 78.221 4.257 Just because I have to put something. <Response [400]>\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for idx, (images, _) in enumerate(dataloader):\n",
    "        recon_images, mu, logvar = vae(images)\n",
    "#         print(images.shape)\n",
    "        \n",
    "        comimg = torch.cat([images * 256.0, recon_images * 256.0])\n",
    "        sample_filename = 'tmp/sample_comp_image.png'\n",
    "        save_image(comimg.data.cpu(), sample_filename)\n",
    "        \n",
    "#         display(Image(sample_filename, width=300, unconfined=True))\n",
    "        \n",
    "        loss, bce, kld = loss_fn(recon_images, images, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#         to_print = \"Epoch[{}/{}] Loss: {:.3f} {:.3f} {:.3f}\".format(epoch+1, \n",
    "#                                 epochs, loss.data[0]/bs, bce.data[0]/bs, kld.data[0]/bs)\n",
    "        to_print = \"Epoch[{}/{}] Loss: {:.3f} {:.3f} {:.3f}\".format(epoch+1, \n",
    "                                epochs, loss.data/bs, bce.data/bs, kld.data/bs)\n",
    "    if epoch % 5 == 0 and epoch != 0:\n",
    "        torch.save(vae.state_dict(), 'models/intermediate/cvae.{}-imgs_{}-epch_{}-{}'.format(model_name, len(dataset.imgs), epoch, epochs))\n",
    "        \n",
    "    print(to_print, loss.data, kld.data, bs)\n",
    "\n",
    "# notify to android when finished training\n",
    "notify(to_print, priority=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'models/cvae.{}-imgs_{}-epch_{}'.format(model_name, len(dataset.imgs), epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
