{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import PIL\n",
    "\n",
    "import torchvision as tv\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "\n",
    "from pushover import notify\n",
    "from utils import makegif\n",
    "from random import randint\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32 # batchsize 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2297, 72)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "\n",
    "def print_gt_zero_elem(matrix):\n",
    "    print(matrix[matrix > 0])\n",
    "    \n",
    "# def load_img(img_path):\n",
    "#     img = PIL.Image.open(img_path)\n",
    "# #     img = PIL.ImageMath.eval(\"int(img)\", img=img)\n",
    "# #     print(img)\n",
    "# #     img = PIL.ImageOps.grayscale(img)\n",
    "#     img = img.convert(mode=\"L\")\n",
    "#     npimg = np.array(img)\n",
    "#     npimg = npimg/256.0\n",
    "#     return npimg\n",
    "# IMRANGE = 256 # uint8\n",
    "# # dataset = datasets.ImageFolder(root='trainings/rolls_gray', transform=transforms.Compose([\n",
    "# # #     transforms.Resize(64),\n",
    "# #     transforms.ToTensor(), \n",
    "# # #     lambda x: (x * IMRANGE).type(torch.IntTensor),\n",
    "# #     lambda x: x.type(torch.FloatTensor),\n",
    "# # ]), loader=load_img)\n",
    "\n",
    "# dataset = datasets.ImageFolder(root='trainings/rolls_gray', transform=transforms.Compose([\n",
    "#     transforms.ToTensor(), \n",
    "#     lambda x: x.type(torch.FloatTensor),\n",
    "# ]), loader=load_img)\n",
    "\n",
    "def load_img(img_path):\n",
    "    img = PIL.Image.open(img_path)\n",
    "    img = img.convert(mode=\"L\")\n",
    "#     npimg = np.array(img)/256.0\n",
    "    return img\n",
    "\n",
    "IMRANGE = 256 # uint8\n",
    "\n",
    "dataset = datasets.ImageFolder(root='trainings/rolls_gray', transform=transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    lambda x: x > 0,\n",
    "    lambda x: x.type(torch.FloatTensor),\n",
    "]), loader=load_img)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "len(dataset.imgs), len(dataloader)\n",
    "\n",
    "# size of input = 3 x 128 x 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed input for debugging\n",
    "fixed_x, _ = next(iter(dataloader))\n",
    "save_image(fixed_x, 'outputs/real_image.png')\n",
    "\n",
    "# Image('outputs/real_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 128])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][0].shape)\n",
    "sample_dat = dataset[0][0]\n",
    "print_gt_zero_elem(sample_dat)\n",
    "HSIZE = 2048 #9216 # 1024\n",
    "ZDIM = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "#         print(\"flatten: \", input.shape)\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=HSIZE):\n",
    "        return input.view(input.size(0), size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=1, h_dim=HSIZE, z_dim=ZDIM):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2), # -> [32, 32, 31, 31] 63\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # -> [32, 64, 14, 14] 31\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2), # -> [32, 128, 6, 6] 14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2), # -> [32, 256, 2, 2] 6\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2), # -> Null -> [32, 512, 2, 2] \n",
    "            nn.ReLU(), \n",
    "            Flatten() # -> [32, 1024]  -> [32, 2048]\n",
    "            # [32, a, b, c] -> [32, abc]\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(), \n",
    "            nn.ConvTranspose2d(h_dim, 256 , kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             UnFlatten(),\n",
    "#             nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "#             nn.Sigmoid(),\n",
    "#         )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "#         print(\"bottle: \",mu.shape, logvar.shape)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "#         print(\"======== Encode ========\", x.shape)\n",
    "        h = self.encoder(x)\n",
    "#         print(\"enc(x): \", h.shape)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "#         print(\"z.shape: \", z.shape)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "#         print(\"======== Decode ========\", z.shape)\n",
    "        z = self.fc3(z)\n",
    "#         print(\"fc3(z).shape: \", z.shape)\n",
    "        z = self.decoder(z)\n",
    "#         print(\"decode(fc3(z)).shape: \", z.shape)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "#         print(z.shape)\n",
    "        z = self.decode(z)\n",
    "#         print(z.shape, mu.shape, logvar.shape)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channels = fixed_x.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(image_channels=image_channels).to(device)\n",
    "# model.load_state_dict(torch.load('vae.torch', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "#     BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD, BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"graybin_bce_d16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100] Loss: 1022.473 1022.422 0.050 tensor(32719.1230) tensor(1.6123) 32\n",
      "Epoch[2/100] Loss: 981.394 981.290 0.104 tensor(31404.6074) tensor(3.3354) 32\n",
      "Epoch[3/100] Loss: 867.052 866.868 0.183 tensor(27745.6562) tensor(5.8669) 32\n",
      "Epoch[4/100] Loss: 1002.440 1002.264 0.175 tensor(32078.0684) tensor(5.6124) 32\n",
      "Epoch[5/100] Loss: 894.700 894.469 0.232 tensor(28630.4082) tensor(7.4089) 32\n",
      "Epoch[6/100] Loss: 958.333 958.117 0.216 tensor(30666.6582) tensor(6.9247) 32\n",
      "Epoch[7/100] Loss: 774.296 774.063 0.233 tensor(24777.4707) tensor(7.4503) 32\n",
      "Epoch[8/100] Loss: 741.276 741.032 0.243 tensor(23720.8223) tensor(7.7861) 32\n",
      "Epoch[9/100] Loss: 731.365 731.101 0.264 tensor(23403.6699) tensor(8.4423) 32\n",
      "Epoch[10/100] Loss: 709.043 708.793 0.250 tensor(22689.3750) tensor(8.0074) 32\n",
      "Epoch[11/100] Loss: 665.015 664.748 0.267 tensor(21280.4668) tensor(8.5294) 32\n",
      "Epoch[12/100] Loss: 609.271 609.038 0.232 tensor(19496.6582) tensor(7.4290) 32\n",
      "Epoch[13/100] Loss: 531.731 531.469 0.262 tensor(17015.3848) tensor(8.3860) 32\n",
      "Epoch[14/100] Loss: 626.180 625.901 0.279 tensor(20037.7715) tensor(8.9236) 32\n",
      "Epoch[15/100] Loss: 515.033 514.736 0.297 tensor(16481.0547) tensor(9.4946) 32\n",
      "Epoch[16/100] Loss: 475.266 474.990 0.276 tensor(15208.4961) tensor(8.8199) 32\n",
      "Epoch[17/100] Loss: 458.548 458.210 0.338 tensor(14673.5498) tensor(10.8207) 32\n",
      "Epoch[18/100] Loss: 451.229 450.911 0.318 tensor(14439.3291) tensor(10.1888) 32\n",
      "Epoch[19/100] Loss: 440.151 439.817 0.334 tensor(14084.8389) tensor(10.7004) 32\n",
      "Epoch[20/100] Loss: 470.045 469.681 0.365 tensor(15041.4551) tensor(11.6649) 32\n",
      "Epoch[21/100] Loss: 425.007 424.687 0.320 tensor(13600.2354) tensor(10.2556) 32\n",
      "Epoch[22/100] Loss: 351.273 350.893 0.380 tensor(11240.7266) tensor(12.1515) 32\n",
      "Epoch[23/100] Loss: 337.725 337.330 0.395 tensor(10807.1992) tensor(12.6396) 32\n",
      "Epoch[24/100] Loss: 303.201 302.774 0.427 tensor(9702.4336) tensor(13.6630) 32\n",
      "Epoch[25/100] Loss: 202.109 201.578 0.531 tensor(6467.4912) tensor(17.0051) 32\n",
      "Epoch[26/100] Loss: 289.052 288.631 0.420 tensor(9249.6543) tensor(13.4474) 32\n",
      "Epoch[27/100] Loss: 220.268 219.776 0.491 tensor(7048.5728) tensor(15.7266) 32\n",
      "Epoch[28/100] Loss: 209.482 208.994 0.488 tensor(6703.4277) tensor(15.6124) 32\n",
      "Epoch[29/100] Loss: 180.330 179.852 0.478 tensor(5770.5640) tensor(15.3084) 32\n",
      "Epoch[30/100] Loss: 224.042 223.571 0.470 tensor(7169.3359) tensor(15.0535) 32\n",
      "Epoch[31/100] Loss: 204.807 204.302 0.505 tensor(6553.8174) tensor(16.1526) 32\n",
      "Epoch[32/100] Loss: 194.608 194.075 0.533 tensor(6227.4604) tensor(17.0543) 32\n",
      "Epoch[33/100] Loss: 185.073 184.511 0.562 tensor(5922.3501) tensor(18.0000) 32\n",
      "Epoch[34/100] Loss: 162.628 162.053 0.576 tensor(5204.1050) tensor(18.4210) 32\n",
      "Epoch[35/100] Loss: 145.588 145.057 0.530 tensor(4658.8037) tensor(16.9732) 32\n",
      "Epoch[36/100] Loss: 113.608 112.981 0.627 tensor(3635.4619) tensor(20.0666) 32\n",
      "Epoch[37/100] Loss: 130.991 130.476 0.515 tensor(4191.7114) tensor(16.4682) 32\n",
      "Epoch[38/100] Loss: 110.716 110.131 0.585 tensor(3542.9143) tensor(18.7225) 32\n",
      "Epoch[39/100] Loss: 118.706 118.155 0.551 tensor(3798.6006) tensor(17.6257) 32\n",
      "Epoch[40/100] Loss: 104.342 103.807 0.535 tensor(3338.9446) tensor(17.1180) 32\n",
      "Epoch[41/100] Loss: 125.945 125.384 0.560 tensor(4030.2253) tensor(17.9306) 32\n",
      "Epoch[42/100] Loss: 101.821 101.256 0.565 tensor(3258.2651) tensor(18.0835) 32\n",
      "Epoch[43/100] Loss: 100.361 99.830 0.532 tensor(3211.5569) tensor(17.0110) 32\n",
      "Epoch[44/100] Loss: 100.835 100.311 0.523 tensor(3226.7048) tensor(16.7495) 32\n",
      "Epoch[45/100] Loss: 99.008 98.432 0.576 tensor(3168.2595) tensor(18.4204) 32\n",
      "Epoch[46/100] Loss: 92.914 92.396 0.518 tensor(2973.2454) tensor(16.5791) 32\n",
      "Epoch[47/100] Loss: 99.185 98.679 0.506 tensor(3173.9351) tensor(16.2006) 32\n",
      "Epoch[48/100] Loss: 87.625 87.051 0.574 tensor(2804.0037) tensor(18.3714) 32\n",
      "Epoch[49/100] Loss: 120.083 119.658 0.425 tensor(3842.6523) tensor(13.6032) 32\n",
      "Epoch[50/100] Loss: 82.940 82.506 0.434 tensor(2654.0933) tensor(13.8950) 32\n",
      "Epoch[51/100] Loss: 75.928 75.408 0.519 tensor(2429.6902) tensor(16.6231) 32\n",
      "Epoch[52/100] Loss: 81.048 80.614 0.434 tensor(2593.5266) tensor(13.8907) 32\n",
      "Epoch[53/100] Loss: 72.597 72.067 0.530 tensor(2323.1030) tensor(16.9534) 32\n",
      "Epoch[54/100] Loss: 64.995 64.530 0.465 tensor(2079.8291) tensor(14.8801) 32\n",
      "Epoch[55/100] Loss: 53.235 52.754 0.480 tensor(1703.5049) tensor(15.3744) 32\n",
      "Epoch[56/100] Loss: 69.512 69.047 0.465 tensor(2224.3748) tensor(14.8658) 32\n",
      "Epoch[57/100] Loss: 61.353 60.781 0.572 tensor(1963.2964) tensor(18.3115) 32\n",
      "Epoch[58/100] Loss: 46.436 45.948 0.488 tensor(1485.9427) tensor(15.6116) 32\n",
      "Epoch[59/100] Loss: 58.141 57.693 0.449 tensor(1860.5164) tensor(14.3521) 32\n",
      "Epoch[60/100] Loss: 64.824 64.371 0.454 tensor(2074.3765) tensor(14.5180) 32\n",
      "Epoch[61/100] Loss: 51.889 51.455 0.434 tensor(1660.4485) tensor(13.8977) 32\n",
      "Epoch[62/100] Loss: 60.101 59.656 0.445 tensor(1923.2397) tensor(14.2336) 32\n",
      "Epoch[63/100] Loss: 65.835 65.413 0.422 tensor(2106.7258) tensor(13.5031) 32\n",
      "Epoch[64/100] Loss: 63.463 63.036 0.427 tensor(2030.8120) tensor(13.6488) 32\n",
      "Epoch[65/100] Loss: 49.551 49.125 0.426 tensor(1585.6432) tensor(13.6408) 32\n",
      "Epoch[66/100] Loss: 53.832 53.434 0.398 tensor(1722.6190) tensor(12.7214) 32\n",
      "Epoch[67/100] Loss: 49.889 49.519 0.370 tensor(1596.4552) tensor(11.8502) 32\n",
      "Epoch[68/100] Loss: 67.331 66.942 0.389 tensor(2154.5857) tensor(12.4436) 32\n",
      "Epoch[69/100] Loss: 59.948 59.517 0.431 tensor(1918.3308) tensor(13.7878) 32\n",
      "Epoch[70/100] Loss: 77.529 77.121 0.408 tensor(2480.9304) tensor(13.0424) 32\n",
      "Epoch[71/100] Loss: 59.683 59.313 0.370 tensor(1909.8550) tensor(11.8362) 32\n",
      "Epoch[72/100] Loss: 51.356 50.959 0.397 tensor(1643.4056) tensor(12.7074) 32\n",
      "Epoch[73/100] Loss: 39.866 39.472 0.394 tensor(1275.7135) tensor(12.6123) 32\n",
      "Epoch[74/100] Loss: 45.177 44.821 0.357 tensor(1445.6711) tensor(11.4130) 32\n",
      "Epoch[75/100] Loss: 51.050 50.674 0.376 tensor(1633.5885) tensor(12.0357) 32\n",
      "Epoch[76/100] Loss: 72.697 72.377 0.320 tensor(2326.2898) tensor(10.2357) 32\n",
      "Epoch[77/100] Loss: 61.694 61.363 0.331 tensor(1974.2017) tensor(10.5925) 32\n",
      "Epoch[78/100] Loss: 59.917 59.589 0.328 tensor(1917.3311) tensor(10.4812) 32\n",
      "Epoch[79/100] Loss: 46.735 46.403 0.331 tensor(1495.5040) tensor(10.5954) 32\n",
      "Epoch[80/100] Loss: 24.212 23.841 0.371 tensor(774.7726) tensor(11.8600) 32\n",
      "Epoch[81/100] Loss: 29.538 29.151 0.387 tensor(945.2115) tensor(12.3832) 32\n",
      "Epoch[82/100] Loss: 34.068 33.648 0.420 tensor(1090.1836) tensor(13.4556) 32\n",
      "Epoch[83/100] Loss: 35.178 34.790 0.388 tensor(1125.6996) tensor(12.4206) 32\n",
      "Epoch[84/100] Loss: 25.432 25.022 0.411 tensor(813.8305) tensor(13.1405) 32\n",
      "Epoch[85/100] Loss: 29.545 29.158 0.387 tensor(945.4327) tensor(12.3923) 32\n",
      "Epoch[86/100] Loss: 33.086 32.717 0.369 tensor(1058.7628) tensor(11.8151) 32\n",
      "Epoch[87/100] Loss: 39.971 39.599 0.372 tensor(1279.0695) tensor(11.8981) 32\n",
      "Epoch[88/100] Loss: 52.792 52.505 0.287 tensor(1689.3473) tensor(9.1772) 32\n",
      "Epoch[89/100] Loss: 51.091 50.815 0.276 tensor(1634.9216) tensor(8.8350) 32\n",
      "Epoch[90/100] Loss: 57.966 57.667 0.299 tensor(1854.9176) tensor(9.5733) 32\n",
      "Epoch[91/100] Loss: 42.478 42.170 0.307 tensor(1359.2908) tensor(9.8354) 32\n",
      "Epoch[92/100] Loss: 52.672 52.391 0.281 tensor(1685.5137) tensor(9.0055) 32\n",
      "Epoch[93/100] Loss: 43.447 43.159 0.288 tensor(1390.2915) tensor(9.2185) 32\n",
      "Epoch[94/100] Loss: 42.037 41.752 0.285 tensor(1345.1852) tensor(9.1240) 32\n",
      "Epoch[95/100] Loss: 44.425 44.139 0.286 tensor(1421.6115) tensor(9.1665) 32\n",
      "Epoch[96/100] Loss: 34.721 34.410 0.311 tensor(1111.0582) tensor(9.9446) 32\n",
      "Epoch[97/100] Loss: 26.972 26.635 0.338 tensor(863.1173) tensor(10.8002) 32\n",
      "Epoch[98/100] Loss: 27.940 27.657 0.283 tensor(894.0865) tensor(9.0513) 32\n",
      "Epoch[99/100] Loss: 22.050 21.700 0.349 tensor(705.5853) tensor(11.1721) 32\n",
      "Epoch[100/100] Loss: 23.225 22.887 0.339 tensor(743.2123) tensor(10.8354) 32\n",
      "Notifing Epoch[100/100] Loss: 23.225 22.887 0.339 Just because I have to put something. <Response [400]>\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for idx, (images, _) in enumerate(dataloader):\n",
    "        recon_images, mu, logvar = vae(images)\n",
    "#         print(images.shape)\n",
    "        \n",
    "        comimg = torch.cat([images * 256.0, recon_images * 256.0])\n",
    "        sample_filename = 'tmp/sample_comp_image.png'\n",
    "        save_image(comimg.data.cpu(), sample_filename)\n",
    "        \n",
    "#         display(Image(sample_filename, width=300, unconfined=True))\n",
    "        \n",
    "        loss, bce, kld = loss_fn(recon_images, images, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#         to_print = \"Epoch[{}/{}] Loss: {:.3f} {:.3f} {:.3f}\".format(epoch+1, \n",
    "#                                 epochs, loss.data[0]/bs, bce.data[0]/bs, kld.data[0]/bs)\n",
    "        to_print = \"Epoch[{}/{}] Loss: {:.3f} {:.3f} {:.3f}\".format(epoch+1, \n",
    "                                epochs, loss.data/bs, bce.data/bs, kld.data/bs)\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        torch.save(vae.state_dict(), 'models/intermediate/cvae.{}-imgs_{}-epch_{}-{}'.format(model_name, len(dataset.imgs), epoch, epochs))\n",
    "        \n",
    "    print(to_print, loss.data, kld.data, bs)\n",
    "\n",
    "# notify to android when finished training\n",
    "notify(to_print, priority=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'models/cvae.{}-imgs_{}-epch_{}'.format(model_name, len(dataset.imgs), epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
