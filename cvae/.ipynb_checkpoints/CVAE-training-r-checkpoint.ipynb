{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import PIL\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "\n",
    "from pushover import notify\n",
    "from utils import makegif\n",
    "from random import randint\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32 # batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1339, 42)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "dataset = datasets.ImageFolder(root='trainings/roll_imgs_velo_partial', transform=transforms.Compose([\n",
    "#     transforms.Resize(64),\n",
    "    transforms.ToTensor(), \n",
    "]))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "len(dataset.imgs), len(dataloader)\n",
    "# size of input = 3 x 128 x 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed input for debugging\n",
    "fixed_x, _ = next(iter(dataloader))\n",
    "save_image(fixed_x, 'outputs/real_image.png')\n",
    "\n",
    "# Image('outputs/real_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 128])\n",
      "tensor([])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[1][0].shape)\n",
    "sample = dataset[1][0]\n",
    "print(sample[sample<0])\n",
    "HSIZE = 2048 #9216 # 1024\n",
    "ZDIM =  32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "#         print(\"flatten: \", input.shape)\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=HSIZE):\n",
    "        return input.view(input.size(0), size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=HSIZE, z_dim=ZDIM):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2), # -> [32, 32, 31, 31] 63\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # -> [32, 64, 14, 14] 31\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2), # -> [32, 128, 6, 6] 14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2), # -> [32, 256, 2, 2] 6\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2), # -> Null -> [32, 512, 2, 2] \n",
    "            nn.ReLU(), \n",
    "            Flatten() # -> [32, 1024]  -> [32, 2048]\n",
    "            # [32, a, b, c] -> [32, abc]\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(), \n",
    "            nn.ConvTranspose2d(h_dim, 256 , kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             UnFlatten(),\n",
    "#             nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "#             nn.Sigmoid(),\n",
    "#         )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "#         print(\"bottle: \",mu.shape, logvar.shape)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "#         print(\"======== Encode ========\", x.shape)\n",
    "        h = self.encoder(x)\n",
    "#         print(\"enc(x): \", h.shape)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "#         print(\"z.shape: \", z.shape)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "#         print(\"======== Decode ========\", z.shape)\n",
    "        z = self.fc3(z)\n",
    "#         print(\"fc3(z).shape: \", z.shape)\n",
    "        z = self.decoder(z)\n",
    "#         print(\"decode(fc3(z)).shape: \", z.shape)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "#         print(z.shape)\n",
    "        z = self.decode(z)\n",
    "#         print(z.shape, mu.shape, logvar.shape)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channels = fixed_x.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(image_channels=image_channels).to(device)\n",
    "# model.load_state_dict(torch.load('vae.torch', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "#     BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "    BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD, BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 70\n",
    "model_name = \"rgb_bce\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/70] Loss: 3408.410 3408.347 0.063 tensor(109069.1094) tensor(2.0008) 32\n",
      "Epoch[2/70] Loss: 3076.811 3076.770 0.041 tensor(98457.9453) tensor(1.2999) 32\n",
      "Epoch[3/70] Loss: 2608.240 2608.178 0.061 tensor(83463.6641) tensor(1.9600) 32\n",
      "Epoch[4/70] Loss: 2496.208 2496.116 0.092 tensor(79878.6562) tensor(2.9297) 32\n",
      "Epoch[5/70] Loss: 2744.256 2744.137 0.118 tensor(87816.1875) tensor(3.7874) 32\n",
      "Epoch[6/70] Loss: 2672.077 2671.941 0.136 tensor(85506.4609) tensor(4.3632) 32\n",
      "Epoch[7/70] Loss: 2395.910 2395.758 0.152 tensor(76669.1250) tensor(4.8602) 32\n",
      "Epoch[8/70] Loss: 2293.020 2292.854 0.166 tensor(73376.6328) tensor(5.3067) 32\n",
      "Epoch[9/70] Loss: 2400.094 2399.926 0.168 tensor(76802.9922) tensor(5.3633) 32\n",
      "Epoch[10/70] Loss: 2392.760 2392.553 0.207 tensor(76568.3281) tensor(6.6194) 32\n",
      "Epoch[11/70] Loss: 2260.603 2260.382 0.221 tensor(72339.2891) tensor(7.0661) 32\n",
      "Epoch[12/70] Loss: 2244.418 2244.175 0.243 tensor(71821.3672) tensor(7.7831) 32\n",
      "Epoch[13/70] Loss: 2199.481 2199.208 0.273 tensor(70383.3984) tensor(8.7418) 32\n",
      "Epoch[14/70] Loss: 2148.178 2147.891 0.287 tensor(68741.7109) tensor(9.1990) 32\n",
      "Epoch[15/70] Loss: 2072.968 2072.671 0.297 tensor(66334.9766) tensor(9.5049) 32\n",
      "Epoch[16/70] Loss: 2301.430 2301.175 0.256 tensor(73645.7734) tensor(8.1897) 32\n",
      "Epoch[17/70] Loss: 1925.108 1924.806 0.303 tensor(61603.4688) tensor(9.6874) 32\n",
      "Epoch[18/70] Loss: 2107.559 2107.265 0.293 tensor(67441.8828) tensor(9.3898) 32\n",
      "Epoch[19/70] Loss: 1604.651 1604.276 0.374 tensor(51348.8242) tensor(11.9794) 32\n",
      "Epoch[20/70] Loss: 1851.127 1850.785 0.342 tensor(59236.0703) tensor(10.9438) 32\n",
      "Epoch[21/70] Loss: 1710.143 1709.815 0.328 tensor(54724.5742) tensor(10.5057) 32\n",
      "Epoch[22/70] Loss: 1501.817 1501.406 0.411 tensor(48058.1562) tensor(13.1573) 32\n",
      "Epoch[23/70] Loss: 1411.128 1410.651 0.476 tensor(45156.0859) tensor(15.2391) 32\n",
      "Epoch[24/70] Loss: 1521.715 1521.287 0.428 tensor(48694.8867) tensor(13.7039) 32\n",
      "Epoch[25/70] Loss: 1295.446 1295.060 0.386 tensor(41454.2617) tensor(12.3476) 32\n",
      "Epoch[26/70] Loss: 1315.272 1314.796 0.476 tensor(42088.6914) tensor(15.2239) 32\n",
      "Epoch[27/70] Loss: 1180.691 1180.193 0.497 tensor(37782.0977) tensor(15.9191) 32\n",
      "Epoch[28/70] Loss: 1450.172 1449.745 0.427 tensor(46405.5039) tensor(13.6722) 32\n",
      "Epoch[29/70] Loss: 1345.176 1344.696 0.479 tensor(43045.6289) tensor(15.3437) 32\n",
      "Epoch[30/70] Loss: 1141.332 1140.774 0.558 tensor(36522.6133) tensor(17.8496) 32\n",
      "Epoch[31/70] Loss: 963.222 962.575 0.647 tensor(30823.1152) tensor(20.7070) 32\n",
      "Epoch[32/70] Loss: 1077.064 1076.428 0.636 tensor(34466.0508) tensor(20.3480) 32\n",
      "Epoch[33/70] Loss: 946.155 945.543 0.611 tensor(30276.9492) tensor(19.5582) 32\n",
      "Epoch[34/70] Loss: 897.629 896.936 0.693 tensor(28724.1309) tensor(22.1850) 32\n",
      "Epoch[35/70] Loss: 919.967 919.317 0.649 tensor(29438.9414) tensor(20.7830) 32\n",
      "Epoch[36/70] Loss: 940.687 940.021 0.667 tensor(30101.9941) tensor(21.3293) 32\n",
      "Epoch[37/70] Loss: 840.750 840.063 0.688 tensor(26904.0059) tensor(22.0030) 32\n",
      "Epoch[38/70] Loss: 878.332 877.659 0.673 tensor(28106.6270) tensor(21.5323) 32\n",
      "Epoch[39/70] Loss: 829.668 828.901 0.767 tensor(26549.3867) tensor(24.5414) 32\n",
      "Epoch[40/70] Loss: 752.726 751.923 0.803 tensor(24087.2344) tensor(25.7008) 32\n",
      "Epoch[41/70] Loss: 838.206 837.419 0.787 tensor(26822.5859) tensor(25.1929) 32\n",
      "Epoch[42/70] Loss: 780.806 780.113 0.693 tensor(24985.7949) tensor(22.1919) 32\n",
      "Epoch[43/70] Loss: 710.590 709.708 0.883 tensor(22738.8945) tensor(28.2513) 32\n",
      "Epoch[44/70] Loss: 748.170 747.374 0.796 tensor(23941.4492) tensor(25.4724) 32\n",
      "Epoch[45/70] Loss: 699.042 698.187 0.855 tensor(22369.3340) tensor(27.3482) 32\n",
      "Epoch[46/70] Loss: 639.821 638.997 0.824 tensor(20474.2754) tensor(26.3558) 32\n",
      "Epoch[47/70] Loss: 667.443 666.592 0.851 tensor(21358.1758) tensor(27.2184) 32\n",
      "Epoch[48/70] Loss: 592.451 591.487 0.964 tensor(18958.4277) tensor(30.8332) 32\n",
      "Epoch[49/70] Loss: 598.614 597.612 1.003 tensor(19155.6621) tensor(32.0914) 32\n",
      "Epoch[50/70] Loss: 586.613 585.684 0.929 tensor(18771.6191) tensor(29.7266) 32\n",
      "Epoch[51/70] Loss: 518.009 517.023 0.985 tensor(16576.2754) tensor(31.5294) 32\n",
      "Epoch[52/70] Loss: 643.886 642.926 0.961 tensor(20604.3613) tensor(30.7430) 32\n",
      "Epoch[53/70] Loss: 673.411 672.519 0.892 tensor(21549.1387) tensor(28.5378) 32\n",
      "Epoch[54/70] Loss: 593.813 592.851 0.962 tensor(19002.0039) tensor(30.7802) 32\n",
      "Epoch[55/70] Loss: 664.156 663.263 0.893 tensor(21253.0020) tensor(28.5803) 32\n",
      "Epoch[56/70] Loss: 555.999 555.016 0.983 tensor(17791.9668) tensor(31.4497) 32\n",
      "Epoch[57/70] Loss: 564.169 563.215 0.954 tensor(18053.4102) tensor(30.5422) 32\n",
      "Epoch[58/70] Loss: 580.912 579.993 0.919 tensor(18589.1777) tensor(29.4036) 32\n",
      "Epoch[59/70] Loss: 526.523 525.567 0.957 tensor(16848.7422) tensor(30.6088) 32\n",
      "Epoch[60/70] Loss: 518.409 517.526 0.882 tensor(16589.0781) tensor(28.2336) 32\n",
      "Epoch[61/70] Loss: 549.769 548.691 1.078 tensor(17592.6191) tensor(34.4991) 32\n",
      "Epoch[62/70] Loss: 445.115 444.027 1.088 tensor(14243.6934) tensor(34.8205) 32\n",
      "Epoch[63/70] Loss: 485.041 483.935 1.106 tensor(15521.3135) tensor(35.3881) 32\n",
      "Epoch[64/70] Loss: 509.192 508.316 0.876 tensor(16294.1455) tensor(28.0339) 32\n",
      "Epoch[65/70] Loss: 556.251 555.351 0.900 tensor(17800.0195) tensor(28.8017) 32\n",
      "Epoch[66/70] Loss: 548.935 548.036 0.898 tensor(17565.9121) tensor(28.7507) 32\n",
      "Epoch[67/70] Loss: 549.628 548.746 0.882 tensor(17588.0898) tensor(28.2116) 32\n",
      "Epoch[68/70] Loss: 584.541 583.618 0.922 tensor(18705.2969) tensor(29.5062) 32\n",
      "Epoch[69/70] Loss: 566.455 565.699 0.756 tensor(18126.5586) tensor(24.1974) 32\n",
      "Epoch[70/70] Loss: 630.904 630.085 0.819 tensor(20188.9258) tensor(26.2179) 32\n",
      "Notifing Epoch[70/70] Loss: 630.904 630.085 0.819 Just because I have to put something. <Response [400]>\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for idx, (images, _) in enumerate(dataloader):\n",
    "        recon_images, mu, logvar = vae(images)\n",
    "#         print(images)\n",
    "        loss, bce, kld = loss_fn(recon_images, images, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#         to_print = \"Epoch[{}/{}] Loss: {:.3f} {:.3f} {:.3f}\".format(epoch+1, \n",
    "#                                 epochs, loss.data[0]/bs, bce.data[0]/bs, kld.data[0]/bs)\n",
    "        to_print = \"Epoch[{}/{}] Loss: {:.3f} {:.3f} {:.3f}\".format(epoch+1, \n",
    "                                epochs, loss.data/bs, bce.data/bs, kld.data/bs)\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(vae.state_dict(), 'models/cvae.{}-imgs_{}-epch_{}-{}'.format(model_name, len(dataset.imgs), epoch, epochs))\n",
    "    print(to_print, loss.data, kld.data, bs)\n",
    "\n",
    "# notify to android when finished training\n",
    "notify(to_print, priority=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'models/cvae.{}-imgs_{}-epch_{}'.format(model_name, len(dataset.imgs), epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
