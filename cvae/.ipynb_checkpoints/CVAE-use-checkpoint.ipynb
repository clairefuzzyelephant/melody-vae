{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "\n",
    "from pushover import notify\n",
    "from utils import makegif\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "import IPython\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "import PIL\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import piano_roll_utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32 # batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "dataset = datasets.ImageFolder(root='trainings/roll_rgb_test', transform=transforms.Compose([\n",
    "#     transforms.Resize(64),\n",
    "    transforms.ToTensor(), \n",
    "]))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "len(dataset.imgs), len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAA80lEQVR4nO3aobGDUBAF0Pc6oAckBeAjkRRAcRSAROIQFIBMD5Fx+SIOftxOyMA5cndmd82dJyAlAAAAAAAAAAAAAAAAAAAAAAAAAAAAAH5FPnD3MAyB09q2DZzGVeSUUt/3+0bXdZvKNE2xu2+3W+C02Dy9SdX55WVZ6rreN/ax2Gfi9AJD/ynuRz7ClzLP89EnAAAAAAAAwAGO/CR5v98Dp5VlGTiNq8gppXVd942qqjaVx+MRu7soisBpsXl6k6rzy6/XK+d/XoJxHDeVpmm+ctIPCQz9p7j7L+hLns/n0ScAAAAAAAAAAAAAAAAAAAT6A73KL0A6uTVxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixed input for debugging\n",
    "fixed_x, _ = next(iter(dataloader))\n",
    "\n",
    "\n",
    "save_image(fixed_x, 'tmp/real_image.png')\n",
    "\n",
    "Image('tmp/real_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][0].shape)\n",
    "HSIZE = 2048 #9216 # 1024\n",
    "ZDIM =  32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gt_zero_elem(matrix):\n",
    "    print(matrix[matrix > 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=HSIZE):\n",
    "        return input.view(input.size(0), size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=HSIZE, z_dim=ZDIM):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2), # -> [32, 32, 31, 31] 63\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # -> [32, 64, 14, 14] 31\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2), # -> [32, 128, 6, 6] 14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2), # -> [32, 256, 2, 2] 6\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2), # -> Null -> [32, 512, 2, 2] \n",
    "            nn.ReLU(), \n",
    "            Flatten() # -> [32, 1024]  -> [32, 2048]\n",
    "            # [32, a, b, c] -> [32, abc]\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(), \n",
    "            nn.ConvTranspose2d(h_dim, 256 , kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             UnFlatten(),\n",
    "#             nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "#             nn.Sigmoid(),\n",
    "#         )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "#         print(\"bottle: \",mu.shape, logvar.shape)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "#         print(\"======== Encode ========\", x.shape)\n",
    "        h = self.encoder(x)\n",
    "#         print(\"enc(x): \", h.shape)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "#         print(\"z.shape: \", z.shape)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "#         print(\"======== Decode ========\", z.shape)\n",
    "        z = self.fc3(z)\n",
    "#         print(\"fc3(z).shape: \", z.shape)\n",
    "        z = self.decoder(z)\n",
    "#         print(\"decode(fc3(z)).shape: \", z.shape)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "#         print(z.shape)\n",
    "        z = self.decode(z)\n",
    "#         print(z.shape, mu.shape, logvar.shape)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channels = fixed_x.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(image_channels=image_channels).to(device)\n",
    "model_version = \"velo-alb-nimgs_1339-epochs_50\" #\"alb-nimgs_4312-epochs_50\" #\"AC-nimgs_2515-epochs_50\" vae.torch-alb-nimgs_4312-epochs_50\n",
    "model.load_state_dict(torch.load('models/vae.torch-' + model_version, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(x):\n",
    "    recon_x, _, _ = model(x)\n",
    "    return torch.cat([x, recon_x])\n",
    "\n",
    "def get_compare_img(x, recon):\n",
    "    return torch.cat([x, recon])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_x = dataset[0][0].unsqueeze(0)\n",
    "fixed_x2 = fixed_x.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAA80lEQVR4nO3aobGDUBAF0Pc6oAckBeAjkRRAcRSAROIQFIBMD5Fx+SIOftxOyMA5cndmd82dJyAlAAAAAAAAAAAAAAAAAAAAAAAAAAAAAH5FPnD3MAyB09q2DZzGVeSUUt/3+0bXdZvKNE2xu2+3W+C02Dy9SdX55WVZ6rreN/ax2Gfi9AJD/ynuRz7ClzLP89EnAAAAAAAAwAGO/CR5v98Dp5VlGTiNq8gppXVd942qqjaVx+MRu7soisBpsXl6k6rzy6/XK+d/XoJxHDeVpmm+ctIPCQz9p7j7L+hLns/n0ScAAAAAAAAAAAAAAAAAAAT6A73KL0A6uTVxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "unconfined": true,
       "width": 300
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rgb_f = 'outputs/rgb-{}.png'.format(model_version)\n",
    "x = fixed_x.data.cpu()\n",
    "x2 = fixed_x2.data.cpu()[0]\n",
    "save_image(x, rgb_f)\n",
    "display(Image(rgb_f, width=300, unconfined=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 128, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1290f6940>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RGB = np.array(x2) #.transpose(1,2,0)\n",
    "# # print(RGB)\n",
    "# R = RGB[0]\n",
    "# G = RGB[1]\n",
    "# B = RGB[2]\n",
    "# L = (0.2989*R + 0.5870*G + 0.1140*B ) * 64\n",
    "# # print(L[L>0])\n",
    "# print(RGB.shape)\n",
    "# plt.gray()\n",
    "# RGB_ = RGB.transpose(1,2,0)\n",
    "# gray_test = plt.imshow(RGB_, cmap=cm.gray)\n",
    "# gray_val = gray_test.get_array().data.transpose(2,0,1) * 128\n",
    "# # print(gray_val[gray_val>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori = PIL.Image.open(rgb_f)\n",
    "ori = ori.convert(mode=\"L\")\n",
    "roll = np.array(ori)/3\n",
    "\n",
    "from skimage import color\n",
    "from skimage import io\n",
    "\n",
    "img = color.rgb2gray(io.imread(rgb_f))\n",
    "\n",
    "midi= piano_roll_utils.piano_roll_to_pretty_midi(roll,fs=50,program=11)\n",
    "IPython.display.Audio(midi.fluidsynth(fs=44100),rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_x = compare(fixed_x)\n",
    "recon_obj, _, _ = model(fixed_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_f = 'outputs/recon-{}.png'.format(model_version)\n",
    "recon = recon_obj.data.cpu()\n",
    "save_image(recon, recon_f)\n",
    "display(Image(recon_f, width=300, unconfined=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon = PIL.Image.open(recon_f)\n",
    "recon = recon.convert(mode=\"L\")\n",
    "roll_res = np.array(recon)/3\n",
    "filt = roll_res > 20\n",
    "roll_res = roll_res * filt\n",
    "\n",
    "filt = roll_res < 75\n",
    "roll_res = roll_res * filt\n",
    "# print(roll_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roll_res[roll_res>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi= piano_roll_utils.piano_roll_to_pretty_midi(roll_res,fs=50,program=11)\n",
    "IPython.display.Audio(midi.fluidsynth(fs=44100),rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PIL.Image.open(\"trainings/roll_gray_test/alb_esp1_format0/st-256.png\")\n",
    "img = img.convert(mode=\"L\")\n",
    "roll_res = np.array(img)\n",
    "print(roll_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi= piano_roll_utils.piano_roll_to_pretty_midi(roll_res,fs=50,program=11)\n",
    "IPython.display.Audio(midi.fluidsynth(fs=44100),rate=44100)\n",
    "# print(roll_res[roll_res>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(fixed_x2, 'tmp/test.png', padding=0)\n",
    "display(Image('tmp/test.png'))\n",
    "z, mu, log_var = model.encode(fixed_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
