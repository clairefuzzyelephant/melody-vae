{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import PIL\n",
    "\n",
    "import torchvision as tv\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "\n",
    "from pushover import notify\n",
    "from utils import makegif\n",
    "from random import randint\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32 # batchsize 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2297, 72)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "\n",
    "def print_gt_zero_elem(matrix):\n",
    "    print(matrix[matrix > 0])\n",
    "    \n",
    "# def load_img(img_path):\n",
    "#     img = PIL.Image.open(img_path)\n",
    "# #     img = PIL.ImageMath.eval(\"int(img)\", img=img)\n",
    "# #     print(img)\n",
    "# #     img = PIL.ImageOps.grayscale(img)\n",
    "#     img = img.convert(mode=\"L\")\n",
    "#     npimg = np.array(img)\n",
    "#     npimg = npimg/256.0\n",
    "#     return npimg\n",
    "# IMRANGE = 256 # uint8\n",
    "# # dataset = datasets.ImageFolder(root='trainings/rolls_gray', transform=transforms.Compose([\n",
    "# # #     transforms.Resize(64),\n",
    "# #     transforms.ToTensor(), \n",
    "# # #     lambda x: (x * IMRANGE).type(torch.IntTensor),\n",
    "# #     lambda x: x.type(torch.FloatTensor),\n",
    "# # ]), loader=load_img)\n",
    "\n",
    "# dataset = datasets.ImageFolder(root='trainings/rolls_gray', transform=transforms.Compose([\n",
    "#     transforms.ToTensor(), \n",
    "#     lambda x: x.type(torch.FloatTensor),\n",
    "# ]), loader=load_img)\n",
    "\n",
    "def load_img(img_path):\n",
    "    img = PIL.Image.open(img_path)\n",
    "    img = img.convert(mode=\"L\")\n",
    "#     npimg = np.array(img)/256.0\n",
    "    return img\n",
    "\n",
    "IMRANGE = 256 # uint8\n",
    "\n",
    "dataset = datasets.ImageFolder(root='trainings/rolls_gray', transform=transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    lambda x: x > 0,\n",
    "    lambda x: x.type(torch.FloatTensor),\n",
    "]), loader=load_img)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "len(dataset.imgs), len(dataloader)\n",
    "\n",
    "# size of input = 3 x 128 x 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed input for debugging\n",
    "fixed_x, _ = next(iter(dataloader))\n",
    "save_image(fixed_x, 'outputs/real_image.png')\n",
    "\n",
    "# Image('outputs/real_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 128])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][0].shape)\n",
    "sample_dat = dataset[0][0]\n",
    "print_gt_zero_elem(sample_dat)\n",
    "HSIZE = 2048 #9216 # 1024\n",
    "ZDIM = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "#         print(\"flatten: \", input.shape)\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=HSIZE):\n",
    "        return input.view(input.size(0), size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=1, h_dim=HSIZE, z_dim=ZDIM):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2), # -> [32, 32, 31, 31] 63\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # -> [32, 64, 14, 14] 31\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2), # -> [32, 128, 6, 6] 14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2), # -> [32, 256, 2, 2] 6\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2), # -> Null -> [32, 512, 2, 2] \n",
    "            nn.ReLU(), \n",
    "            Flatten() # -> [32, 1024]  -> [32, 2048]\n",
    "            # [32, a, b, c] -> [32, abc]\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(), \n",
    "            nn.ConvTranspose2d(h_dim, 256 , kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             UnFlatten(),\n",
    "#             nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "#             nn.Sigmoid(),\n",
    "#         )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "#         print(\"bottle: \",mu.shape, logvar.shape)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "#         print(\"======== Encode ========\", x.shape)\n",
    "        h = self.encoder(x)\n",
    "#         print(\"enc(x): \", h.shape)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "#         print(\"z.shape: \", z.shape)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "#         print(\"======== Decode ========\", z.shape)\n",
    "        z = self.fc3(z)\n",
    "#         print(\"fc3(z).shape: \", z.shape)\n",
    "        z = self.decoder(z)\n",
    "#         print(\"decode(fc3(z)).shape: \", z.shape)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "#         print(z.shape)\n",
    "        z = self.decode(z)\n",
    "#         print(z.shape, mu.shape, logvar.shape)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channels = fixed_x.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(image_channels=image_channels).to(device)\n",
    "# model.load_state_dict(torch.load('vae.torch', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "#     BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD, BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"graybin_bce_d8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100] Loss: 1001.615 1001.567 0.048 tensor(32051.6797) tensor(1.5244) 32\n",
      "Epoch[2/100] Loss: 1125.948 1125.863 0.085 tensor(36030.3359) tensor(2.7131) 32\n",
      "Epoch[3/100] Loss: 1101.579 1101.451 0.128 tensor(35250.5391) tensor(4.0942) 32\n",
      "Epoch[4/100] Loss: 810.175 809.995 0.181 tensor(25925.6074) tensor(5.7826) 32\n",
      "Epoch[5/100] Loss: 897.065 896.897 0.169 tensor(28706.0918) tensor(5.3998) 32\n",
      "Epoch[6/100] Loss: 952.596 952.426 0.170 tensor(30483.0762) tensor(5.4319) 32\n",
      "Epoch[7/100] Loss: 761.914 761.772 0.142 tensor(24381.2363) tensor(4.5285) 32\n",
      "Epoch[8/100] Loss: 759.272 759.132 0.139 tensor(24296.6973) tensor(4.4625) 32\n",
      "Epoch[9/100] Loss: 697.519 697.361 0.158 tensor(22320.6055) tensor(5.0466) 32\n",
      "Epoch[10/100] Loss: 644.665 644.499 0.166 tensor(20629.2734) tensor(5.2971) 32\n",
      "Epoch[11/100] Loss: 665.073 664.917 0.156 tensor(21282.3438) tensor(5.0050) 32\n",
      "Epoch[12/100] Loss: 621.195 621.026 0.169 tensor(19878.2266) tensor(5.4095) 32\n",
      "Epoch[13/100] Loss: 689.271 689.079 0.191 tensor(22056.6602) tensor(6.1189) 32\n",
      "Epoch[14/100] Loss: 582.541 582.367 0.174 tensor(18641.3223) tensor(5.5825) 32\n",
      "Epoch[15/100] Loss: 589.790 589.559 0.231 tensor(18873.2754) tensor(7.3922) 32\n",
      "Epoch[16/100] Loss: 474.737 474.537 0.201 tensor(15191.5889) tensor(6.4172) 32\n",
      "Epoch[17/100] Loss: 536.246 536.025 0.221 tensor(17159.8574) tensor(7.0619) 32\n",
      "Epoch[18/100] Loss: 551.064 550.847 0.216 tensor(17634.0371) tensor(6.9275) 32\n",
      "Epoch[19/100] Loss: 463.266 463.050 0.217 tensor(14824.5273) tensor(6.9285) 32\n",
      "Epoch[20/100] Loss: 457.347 457.113 0.233 tensor(14635.0977) tensor(7.4697) 32\n",
      "Epoch[21/100] Loss: 504.724 504.492 0.232 tensor(16151.1826) tensor(7.4373) 32\n",
      "Epoch[22/100] Loss: 403.358 403.107 0.251 tensor(12907.4551) tensor(8.0328) 32\n",
      "Epoch[23/100] Loss: 444.463 444.217 0.246 tensor(14222.8145) tensor(7.8734) 32\n",
      "Epoch[24/100] Loss: 368.103 367.784 0.318 tensor(11779.2871) tensor(10.1890) 32\n",
      "Epoch[25/100] Loss: 436.730 436.504 0.226 tensor(13975.3613) tensor(7.2228) 32\n",
      "Epoch[26/100] Loss: 397.891 397.603 0.289 tensor(12732.5244) tensor(9.2326) 32\n",
      "Epoch[27/100] Loss: 474.459 474.179 0.280 tensor(15182.6777) tensor(8.9530) 32\n",
      "Epoch[28/100] Loss: 403.504 403.201 0.302 tensor(12912.1162) tensor(9.6692) 32\n",
      "Epoch[29/100] Loss: 390.630 390.372 0.258 tensor(12500.1729) tensor(8.2646) 32\n",
      "Epoch[30/100] Loss: 374.638 374.341 0.297 tensor(11988.4033) tensor(9.5024) 32\n",
      "Epoch[31/100] Loss: 311.381 311.076 0.305 tensor(9964.1934) tensor(9.7617) 32\n",
      "Epoch[32/100] Loss: 405.621 405.370 0.251 tensor(12979.8721) tensor(8.0372) 32\n",
      "Epoch[33/100] Loss: 290.931 290.644 0.288 tensor(9309.8057) tensor(9.2077) 32\n",
      "Epoch[34/100] Loss: 241.634 241.328 0.306 tensor(7732.2891) tensor(9.7790) 32\n",
      "Epoch[35/100] Loss: 312.169 311.880 0.289 tensor(9989.4092) tensor(9.2558) 32\n",
      "Epoch[36/100] Loss: 323.959 323.646 0.313 tensor(10366.6729) tensor(10.0059) 32\n",
      "Epoch[37/100] Loss: 273.940 273.651 0.289 tensor(8766.0801) tensor(9.2403) 32\n",
      "Epoch[38/100] Loss: 276.143 275.868 0.275 tensor(8836.5732) tensor(8.8101) 32\n",
      "Epoch[39/100] Loss: 234.574 234.295 0.279 tensor(7506.3662) tensor(8.9293) 32\n",
      "Epoch[40/100] Loss: 202.789 202.484 0.305 tensor(6489.2568) tensor(9.7760) 32\n",
      "Epoch[41/100] Loss: 195.072 194.772 0.300 tensor(6242.3062) tensor(9.5953) 32\n",
      "Epoch[42/100] Loss: 262.737 262.465 0.272 tensor(8407.5889) tensor(8.7061) 32\n",
      "Epoch[43/100] Loss: 227.630 227.334 0.295 tensor(7284.1504) tensor(9.4488) 32\n",
      "Epoch[44/100] Loss: 209.667 209.393 0.274 tensor(6709.3325) tensor(8.7538) 32\n",
      "Epoch[45/100] Loss: 188.737 188.455 0.282 tensor(6039.5776) tensor(9.0110) 32\n",
      "Epoch[46/100] Loss: 191.832 191.539 0.293 tensor(6138.6294) tensor(9.3752) 32\n",
      "Epoch[47/100] Loss: 198.498 198.142 0.356 tensor(6351.9375) tensor(11.3948) 32\n",
      "Epoch[48/100] Loss: 148.976 148.642 0.334 tensor(4767.2393) tensor(10.6937) 32\n",
      "Epoch[49/100] Loss: 160.183 159.920 0.263 tensor(5125.8501) tensor(8.4056) 32\n",
      "Epoch[50/100] Loss: 129.598 129.243 0.355 tensor(4147.1455) tensor(11.3612) 32\n",
      "Epoch[51/100] Loss: 124.139 123.780 0.359 tensor(3972.4604) tensor(11.4923) 32\n",
      "Epoch[52/100] Loss: 143.306 142.949 0.358 tensor(4585.7983) tensor(11.4401) 32\n",
      "Epoch[53/100] Loss: 131.078 130.715 0.363 tensor(4194.5029) tensor(11.6124) 32\n",
      "Epoch[54/100] Loss: 125.957 125.609 0.348 tensor(4030.6106) tensor(11.1325) 32\n",
      "Epoch[55/100] Loss: 153.900 153.573 0.327 tensor(4924.7954) tensor(10.4699) 32\n",
      "Epoch[56/100] Loss: 158.064 157.713 0.351 tensor(5058.0615) tensor(11.2464) 32\n",
      "Epoch[57/100] Loss: 167.476 167.207 0.269 tensor(5359.2446) tensor(8.6216) 32\n",
      "Epoch[58/100] Loss: 126.974 126.690 0.284 tensor(4063.1682) tensor(9.0817) 32\n",
      "Epoch[59/100] Loss: 145.792 145.460 0.332 tensor(4665.3369) tensor(10.6314) 32\n",
      "Epoch[60/100] Loss: 97.000 96.695 0.305 tensor(3104.0010) tensor(9.7707) 32\n",
      "Epoch[61/100] Loss: 105.150 104.806 0.344 tensor(3364.7954) tensor(11.0152) 32\n",
      "Epoch[62/100] Loss: 85.895 85.510 0.385 tensor(2748.6365) tensor(12.3103) 32\n",
      "Epoch[63/100] Loss: 109.582 109.251 0.331 tensor(3506.6194) tensor(10.5946) 32\n",
      "Epoch[64/100] Loss: 93.179 92.864 0.315 tensor(2981.7368) tensor(10.0872) 32\n",
      "Epoch[65/100] Loss: 100.435 100.093 0.342 tensor(3213.9319) tensor(10.9481) 32\n",
      "Epoch[66/100] Loss: 103.388 103.061 0.327 tensor(3308.4263) tensor(10.4621) 32\n",
      "Epoch[67/100] Loss: 140.120 139.825 0.295 tensor(4483.8418) tensor(9.4529) 32\n",
      "Epoch[68/100] Loss: 101.710 101.375 0.335 tensor(3254.7324) tensor(10.7195) 32\n",
      "Epoch[69/100] Loss: 117.927 117.613 0.314 tensor(3773.6689) tensor(10.0538) 32\n",
      "Epoch[70/100] Loss: 95.964 95.658 0.305 tensor(3070.8430) tensor(9.7751) 32\n",
      "Epoch[71/100] Loss: 108.127 107.817 0.310 tensor(3460.0718) tensor(9.9140) 32\n",
      "Epoch[72/100] Loss: 92.658 92.369 0.289 tensor(2965.0444) tensor(9.2500) 32\n",
      "Epoch[73/100] Loss: 63.785 63.473 0.312 tensor(2041.1134) tensor(9.9710) 32\n",
      "Epoch[74/100] Loss: 99.343 99.021 0.322 tensor(3178.9690) tensor(10.3112) 32\n",
      "Epoch[75/100] Loss: 89.643 89.335 0.308 tensor(2868.5720) tensor(9.8611) 32\n",
      "Epoch[76/100] Loss: 66.298 66.022 0.276 tensor(2121.5518) tensor(8.8339) 32\n",
      "Epoch[77/100] Loss: 82.158 81.855 0.303 tensor(2629.0498) tensor(9.7015) 32\n",
      "Epoch[78/100] Loss: 88.983 88.660 0.323 tensor(2847.4712) tensor(10.3510) 32\n",
      "Epoch[79/100] Loss: 91.046 90.705 0.340 tensor(2913.4614) tensor(10.8945) 32\n",
      "Epoch[80/100] Loss: 117.060 116.784 0.276 tensor(3745.9048) tensor(8.8200) 32\n",
      "Epoch[81/100] Loss: 94.915 94.620 0.294 tensor(3037.2739) tensor(9.4200) 32\n",
      "Epoch[82/100] Loss: 89.690 89.418 0.272 tensor(2870.0779) tensor(8.7016) 32\n",
      "Epoch[83/100] Loss: 78.322 78.037 0.285 tensor(2506.3176) tensor(9.1177) 32\n",
      "Epoch[84/100] Loss: 121.395 121.054 0.340 tensor(3884.6257) tensor(10.8887) 32\n",
      "Epoch[85/100] Loss: 89.385 89.082 0.303 tensor(2860.3259) tensor(9.6943) 32\n",
      "Epoch[86/100] Loss: 68.783 68.448 0.335 tensor(2201.0652) tensor(10.7212) 32\n",
      "Epoch[87/100] Loss: 70.867 70.556 0.311 tensor(2267.7441) tensor(9.9391) 32\n",
      "Epoch[88/100] Loss: 68.241 67.953 0.288 tensor(2183.7029) tensor(9.2131) 32\n",
      "Epoch[89/100] Loss: 45.027 44.715 0.312 tensor(1440.8656) tensor(9.9744) 32\n",
      "Epoch[90/100] Loss: 52.724 52.455 0.269 tensor(1687.1772) tensor(8.6186) 32\n",
      "Epoch[91/100] Loss: 55.677 55.346 0.331 tensor(1781.6713) tensor(10.5887) 32\n",
      "Epoch[92/100] Loss: 57.394 57.095 0.299 tensor(1836.5985) tensor(9.5745) 32\n",
      "Epoch[93/100] Loss: 78.830 78.499 0.330 tensor(2522.5508) tensor(10.5681) 32\n",
      "Epoch[94/100] Loss: 55.170 54.841 0.329 tensor(1765.4379) tensor(10.5358) 32\n",
      "Epoch[95/100] Loss: 54.266 53.961 0.305 tensor(1736.5210) tensor(9.7691) 32\n",
      "Epoch[96/100] Loss: 80.540 80.254 0.285 tensor(2577.2686) tensor(9.1324) 32\n",
      "Epoch[97/100] Loss: 65.019 64.765 0.254 tensor(2080.5974) tensor(8.1279) 32\n",
      "Epoch[98/100] Loss: 72.283 71.951 0.332 tensor(2313.0635) tensor(10.6232) 32\n",
      "Epoch[99/100] Loss: 100.304 100.032 0.272 tensor(3209.7307) tensor(8.6943) 32\n",
      "Epoch[100/100] Loss: 77.907 77.603 0.304 tensor(2493.0129) tensor(9.7323) 32\n",
      "Notifing Epoch[100/100] Loss: 77.907 77.603 0.304 Just because I have to put something. <Response [400]>\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for idx, (images, _) in enumerate(dataloader):\n",
    "        recon_images, mu, logvar = vae(images)\n",
    "#         print(images.shape)\n",
    "        \n",
    "        comimg = torch.cat([images * 256.0, recon_images * 256.0])\n",
    "        sample_filename = 'tmp/sample_comp_image.png'\n",
    "        save_image(comimg.data.cpu(), sample_filename)\n",
    "        \n",
    "#         display(Image(sample_filename, width=300, unconfined=True))\n",
    "        \n",
    "        loss, bce, kld = loss_fn(recon_images, images, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#         to_print = \"Epoch[{}/{}] Loss: {:.3f} {:.3f} {:.3f}\".format(epoch+1, \n",
    "#                                 epochs, loss.data[0]/bs, bce.data[0]/bs, kld.data[0]/bs)\n",
    "        to_print = \"Epoch[{}/{}] Loss: {:.3f} {:.3f} {:.3f}\".format(epoch+1, \n",
    "                                epochs, loss.data/bs, bce.data/bs, kld.data/bs)\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        torch.save(vae.state_dict(), 'models/intermediate/cvae.{}-imgs_{}-epch_{}-{}'.format(model_name, len(dataset.imgs), epoch, epochs))\n",
    "        \n",
    "    print(to_print, loss.data, kld.data, bs)\n",
    "\n",
    "# notify to android when finished training\n",
    "notify(to_print, priority=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'models/cvae.{}-imgs_{}-epch_{}'.format(model_name, len(dataset.imgs), epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
