{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import PIL\n",
    "\n",
    "import torchvision as tv\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "\n",
    "from pushover import notify\n",
    "from utils import makegif\n",
    "from random import randint\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32 # batchsize 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2297, 72)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "\n",
    "def print_gt_zero_elem(matrix):\n",
    "    print(matrix[matrix > 0])\n",
    "\n",
    "def print_elems_gt(matrix, val):\n",
    "    print(matrix[matrix > val])\n",
    "    \n",
    "def load_img(img_path):\n",
    "    img = PIL.Image.open(img_path)\n",
    "    img = img.convert(mode=\"L\")\n",
    "#     npimg = np.array(img)/256.0\n",
    "    return img\n",
    "\n",
    "IMRANGE = 256 # uint8\n",
    "\n",
    "# dataset = datasets.ImageFolder(root='trainings/rolls_gray', transform=transforms.Compose([\n",
    "#     transforms.ToTensor(), \n",
    "#     lambda x: (x > 0).type(torch.FloatTensor) + (x == 0).type(torch.FloatTensor) * -1,\n",
    "# ]), loader=load_img)\n",
    "\n",
    "dataset = datasets.ImageFolder(root='trainings/rolls_gray', transform=transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    lambda x: x > 0,\n",
    "    lambda x: x.type(torch.FloatTensor),\n",
    "]), loader=load_img)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "len(dataset.imgs), len(dataloader)\n",
    "\n",
    "# size of input = 3 x 128 x 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed input for debugging\n",
    "fixed_x, _ = next(iter(dataloader))\n",
    "save_image(fixed_x, 'outputs/real_image.png')\n",
    "\n",
    "# Image('outputs/real_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 128])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][0].shape)\n",
    "sample_dat = dataset[0][0]\n",
    "print(sample_dat)\n",
    "HSIZE = 2048 #9216 # 1024\n",
    "ZDIM = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "#         print(\"flatten: \", input.shape)\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=HSIZE):\n",
    "        return input.view(input.size(0), size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=1, h_dim=HSIZE, z_dim=ZDIM):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2), # -> [32, 32, 31, 31] 63\n",
    "            nn.BatchNorm2d(32),\n",
    "#             nn.Tanh(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # -> [32, 64, 14, 14] 31\n",
    "            nn.BatchNorm2d(64),\n",
    "#             nn.Tanh(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2), # -> [32, 128, 6, 6] 14\n",
    "            nn.BatchNorm2d(128),\n",
    "#             nn.Tanh(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2), # -> [32, 256, 2, 2] 6\n",
    "            nn.BatchNorm2d(256),\n",
    "#             nn.Tanh(),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2), # -> Null -> [32, 512, 2, 2] \n",
    "            nn.BatchNorm2d(512),\n",
    "#             nn.Tanh(),\n",
    "            nn.ReLU(), \n",
    "            Flatten() # -> [32, 1024]  -> [32, 2048]\n",
    "#             # [32, a, b, c] -> [32, abc]\n",
    "        )\n",
    "#         self.relu = nn.Sequential(nn.Tanh())\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(), \n",
    "            nn.ConvTranspose2d(h_dim, 256 , kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "#             nn.Tanh(),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "#             nn.Tanh(),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "#             nn.Tanh(),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "#             nn.Tanh(),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.BatchNorm2d(image_channels),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             UnFlatten(),\n",
    "#             nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "#             nn.Sigmoid(),\n",
    "#         )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "#         print(\"bottle: \",mu.shape, logvar.shape)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "#         print(\"======== Encode ========\", x.shape)\n",
    "        h = self.encoder(x)\n",
    "#         print(\"enc(x): \", h.shape, print_elems_gt(h, 1))\n",
    "#         print(\"relu(enc): \", self.relu(h))\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "#         print(\"z.shape: \", z.shape)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "#         print(\"======== Decode ========\", z.shape)\n",
    "        z = self.fc3(z)\n",
    "#         print(\"fc3(z).shape: \", z.shape)\n",
    "        z = self.decoder(z)\n",
    "#         print(\"decode(fc3(z)).shape: \", z.shape)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "#         print(z.shape)\n",
    "        z = self.decode(z)\n",
    "#         print(z.shape, mu.shape, logvar.shape)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channels = fixed_x.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(image_channels=image_channels).to(device)\n",
    "# model.load_state_dict(torch.load('vae.torch', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "#     BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD, BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"graybin_bce_d16_bn_relu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/300] Loss: 8337.165 8336.990 0.175 tensor(266789.2812) tensor(5.5976) 32\n",
      "Epoch[2/300] Loss: 7907.928 7907.720 0.208 tensor(253053.7031) tensor(6.6513) 32\n",
      "Epoch[3/300] Loss: 7527.611 7527.390 0.221 tensor(240883.5469) tensor(7.0577) 32\n",
      "Epoch[4/300] Loss: 7140.315 7140.078 0.238 tensor(228490.0938) tensor(7.6140) 32\n",
      "Epoch[5/300] Loss: 6671.595 6671.382 0.213 tensor(213491.0312) tensor(6.8123) 32\n",
      "Epoch[6/300] Loss: 6292.237 6291.967 0.270 tensor(201351.5938) tensor(8.6396) 32\n",
      "Epoch[7/300] Loss: 5980.260 5979.969 0.291 tensor(191368.3281) tensor(9.3196) 32\n",
      "Epoch[8/300] Loss: 5680.641 5680.299 0.342 tensor(181780.5000) tensor(10.9282) 32\n",
      "Epoch[9/300] Loss: 5363.354 5363.017 0.337 tensor(171627.3438) tensor(10.7951) 32\n",
      "Epoch[10/300] Loss: 5105.131 5104.771 0.361 tensor(163364.2031) tensor(11.5476) 32\n",
      "Epoch[11/300] Loss: 4742.402 4742.049 0.353 tensor(151756.8750) tensor(11.2919) 32\n",
      "Epoch[12/300] Loss: 4504.812 4504.439 0.372 tensor(144153.9844) tensor(11.9186) 32\n",
      "Epoch[13/300] Loss: 4326.430 4326.051 0.379 tensor(138445.7500) tensor(12.1271) 32\n",
      "Epoch[14/300] Loss: 4135.292 4134.914 0.378 tensor(132329.3281) tensor(12.0921) 32\n",
      "Epoch[15/300] Loss: 4016.477 4016.068 0.409 tensor(128527.2656) tensor(13.0770) 32\n",
      "Epoch[16/300] Loss: 3766.068 3765.675 0.393 tensor(120514.1719) tensor(12.5709) 32\n",
      "Epoch[17/300] Loss: 3450.420 3450.007 0.413 tensor(110413.4453) tensor(13.2114) 32\n",
      "Epoch[18/300] Loss: 3342.159 3341.708 0.451 tensor(106949.0859) tensor(14.4238) 32\n",
      "Epoch[19/300] Loss: 3229.996 3229.566 0.431 tensor(103359.8828) tensor(13.7782) 32\n",
      "Epoch[20/300] Loss: 3086.416 3085.983 0.433 tensor(98765.3203) tensor(13.8635) 32\n",
      "Epoch[21/300] Loss: 2955.085 2954.664 0.421 tensor(94562.7266) tensor(13.4723) 32\n",
      "Epoch[22/300] Loss: 2791.911 2791.506 0.405 tensor(89341.1484) tensor(12.9533) 32\n",
      "Epoch[23/300] Loss: 2576.978 2576.520 0.458 tensor(82463.2969) tensor(14.6449) 32\n",
      "Epoch[24/300] Loss: 2483.400 2482.959 0.441 tensor(79468.8047) tensor(14.1023) 32\n",
      "Epoch[25/300] Loss: 2435.092 2434.613 0.478 tensor(77922.9297) tensor(15.3047) 32\n",
      "Epoch[26/300] Loss: 2308.277 2307.814 0.463 tensor(73864.8594) tensor(14.8171) 32\n",
      "Epoch[27/300] Loss: 2148.760 2148.295 0.466 tensor(68760.3359) tensor(14.8982) 32\n",
      "Epoch[28/300] Loss: 2024.507 2024.018 0.489 tensor(64784.2266) tensor(15.6362) 32\n",
      "Epoch[29/300] Loss: 1976.968 1976.503 0.465 tensor(63262.9844) tensor(14.8957) 32\n",
      "Epoch[30/300] Loss: 2003.031 2002.549 0.481 tensor(64096.9766) tensor(15.3947) 32\n",
      "Epoch[31/300] Loss: 1806.292 1805.829 0.462 tensor(57801.3281) tensor(14.7903) 32\n",
      "Epoch[32/300] Loss: 1756.989 1756.492 0.497 tensor(56223.6328) tensor(15.8965) 32\n",
      "Epoch[33/300] Loss: 1656.949 1656.459 0.491 tensor(53022.3828) tensor(15.7031) 32\n",
      "Epoch[34/300] Loss: 1547.246 1546.753 0.494 tensor(49511.8828) tensor(15.7995) 32\n",
      "Epoch[35/300] Loss: 1538.023 1537.529 0.494 tensor(49216.7266) tensor(15.8098) 32\n",
      "Epoch[36/300] Loss: 1514.749 1514.233 0.516 tensor(48471.9531) tensor(16.5120) 32\n",
      "Epoch[37/300] Loss: 1409.675 1409.174 0.500 tensor(45109.5859) tensor(16.0135) 32\n",
      "Epoch[38/300] Loss: 1495.878 1495.374 0.505 tensor(47868.1055) tensor(16.1464) 32\n",
      "Epoch[39/300] Loss: 1313.000 1312.478 0.522 tensor(42015.9883) tensor(16.6961) 32\n",
      "Epoch[40/300] Loss: 1256.409 1255.905 0.504 tensor(40205.0820) tensor(16.1179) 32\n",
      "Epoch[41/300] Loss: 1239.039 1238.516 0.522 tensor(39649.2344) tensor(16.7193) 32\n",
      "Epoch[42/300] Loss: 1216.822 1216.324 0.498 tensor(38938.3086) tensor(15.9360) 32\n",
      "Epoch[43/300] Loss: 1141.736 1141.206 0.530 tensor(36535.5469) tensor(16.9676) 32\n",
      "Epoch[44/300] Loss: 1097.428 1096.884 0.544 tensor(35117.6953) tensor(17.4135) 32\n",
      "Epoch[45/300] Loss: 1070.762 1070.244 0.518 tensor(34264.3789) tensor(16.5784) 32\n",
      "Epoch[46/300] Loss: 1036.070 1035.567 0.503 tensor(33154.2461) tensor(16.0912) 32\n",
      "Epoch[47/300] Loss: 998.917 998.361 0.556 tensor(31965.3496) tensor(17.7849) 32\n",
      "Epoch[48/300] Loss: 956.860 956.320 0.541 tensor(30619.5332) tensor(17.3036) 32\n",
      "Epoch[49/300] Loss: 894.364 893.801 0.563 tensor(28619.6465) tensor(18.0034) 32\n",
      "Epoch[50/300] Loss: 890.782 890.237 0.545 tensor(28505.0156) tensor(17.4351) 32\n",
      "Epoch[51/300] Loss: 826.927 826.407 0.520 tensor(26461.6484) tensor(16.6332) 32\n",
      "Epoch[52/300] Loss: 851.081 850.531 0.550 tensor(27234.5801) tensor(17.5947) 32\n",
      "Epoch[53/300] Loss: 819.459 818.948 0.511 tensor(26222.7012) tensor(16.3620) 32\n",
      "Epoch[54/300] Loss: 789.842 789.290 0.552 tensor(25274.9531) tensor(17.6628) 32\n",
      "Epoch[55/300] Loss: 820.185 819.636 0.549 tensor(26245.9141) tensor(17.5606) 32\n",
      "Epoch[56/300] Loss: 718.096 717.526 0.570 tensor(22979.0586) tensor(18.2359) 32\n",
      "Epoch[57/300] Loss: 729.316 728.747 0.569 tensor(23338.1074) tensor(18.2015) 32\n",
      "Epoch[58/300] Loss: 798.401 797.845 0.556 tensor(25548.8359) tensor(17.7820) 32\n",
      "Epoch[59/300] Loss: 696.566 695.957 0.609 tensor(22290.0996) tensor(19.4827) 32\n",
      "Epoch[60/300] Loss: 602.697 602.110 0.588 tensor(19286.3184) tensor(18.8004) 32\n",
      "Epoch[61/300] Loss: 626.009 625.450 0.559 tensor(20032.2812) tensor(17.8974) 32\n",
      "Epoch[62/300] Loss: 651.943 651.387 0.555 tensor(20862.1660) tensor(17.7666) 32\n",
      "Epoch[63/300] Loss: 605.303 604.720 0.584 tensor(19369.7031) tensor(18.6773) 32\n",
      "Epoch[64/300] Loss: 550.899 550.297 0.602 tensor(17628.7539) tensor(19.2548) 32\n",
      "Epoch[65/300] Loss: 508.873 508.311 0.561 tensor(16283.9248) tensor(17.9644) 32\n",
      "Epoch[66/300] Loss: 521.552 520.978 0.574 tensor(16689.6660) tensor(18.3576) 32\n",
      "Epoch[67/300] Loss: 529.083 528.503 0.580 tensor(16930.6562) tensor(18.5697) 32\n",
      "Epoch[68/300] Loss: 572.676 572.087 0.589 tensor(18325.6230) tensor(18.8540) 32\n",
      "Epoch[69/300] Loss: 481.458 480.862 0.596 tensor(15406.6553) tensor(19.0676) 32\n",
      "Epoch[70/300] Loss: 481.050 480.450 0.600 tensor(15393.5967) tensor(19.1953) 32\n",
      "Epoch[71/300] Loss: 452.084 451.466 0.618 tensor(14466.6748) tensor(19.7618) 32\n",
      "Epoch[72/300] Loss: 429.827 429.225 0.602 tensor(13754.4648) tensor(19.2656) 32\n",
      "Epoch[73/300] Loss: 417.687 417.075 0.612 tensor(13365.9785) tensor(19.5742) 32\n",
      "Epoch[74/300] Loss: 413.719 413.101 0.618 tensor(13239.0127) tensor(19.7862) 32\n",
      "Epoch[75/300] Loss: 375.895 375.287 0.608 tensor(12028.6396) tensor(19.4488) 32\n",
      "Epoch[76/300] Loss: 393.607 393.042 0.565 tensor(12595.4248) tensor(18.0918) 32\n",
      "Epoch[77/300] Loss: 361.441 360.800 0.640 tensor(11566.1016) tensor(20.4911) 32\n",
      "Epoch[78/300] Loss: 354.855 354.247 0.608 tensor(11355.3633) tensor(19.4505) 32\n",
      "Epoch[79/300] Loss: 358.562 357.943 0.619 tensor(11473.9883) tensor(19.8153) 32\n",
      "Epoch[80/300] Loss: 354.703 354.096 0.607 tensor(11350.5000) tensor(19.4380) 32\n",
      "Epoch[81/300] Loss: 320.459 319.870 0.588 tensor(10254.6729) tensor(18.8304) 32\n",
      "Epoch[82/300] Loss: 315.750 315.125 0.625 tensor(10104.0088) tensor(20.0139) 32\n",
      "Epoch[83/300] Loss: 295.934 295.324 0.611 tensor(9469.9004) tensor(19.5413) 32\n",
      "Epoch[84/300] Loss: 334.382 333.777 0.604 tensor(10700.2139) tensor(19.3353) 32\n",
      "Epoch[85/300] Loss: 285.651 285.025 0.626 tensor(9140.8213) tensor(20.0324) 32\n",
      "Epoch[86/300] Loss: 283.370 282.716 0.654 tensor(9067.8457) tensor(20.9282) 32\n",
      "Epoch[87/300] Loss: 269.106 268.493 0.613 tensor(8611.3867) tensor(19.6051) 32\n",
      "Epoch[88/300] Loss: 258.105 257.490 0.615 tensor(8259.3633) tensor(19.6820) 32\n",
      "Epoch[89/300] Loss: 235.243 234.596 0.647 tensor(7527.7671) tensor(20.6968) 32\n",
      "Epoch[90/300] Loss: 261.153 260.522 0.631 tensor(8356.9082) tensor(20.1932) 32\n",
      "Epoch[91/300] Loss: 246.031 245.399 0.632 tensor(7872.9893) tensor(20.2165) 32\n",
      "Epoch[92/300] Loss: 220.238 219.592 0.646 tensor(7047.6216) tensor(20.6640) 32\n",
      "Epoch[93/300] Loss: 219.766 219.114 0.652 tensor(7032.5229) tensor(20.8670) 32\n",
      "Epoch[94/300] Loss: 213.075 212.425 0.650 tensor(6818.3853) tensor(20.7915) 32\n",
      "Epoch[95/300] Loss: 231.476 230.811 0.665 tensor(7407.2305) tensor(21.2723) 32\n",
      "Epoch[96/300] Loss: 225.731 225.086 0.646 tensor(7223.4009) tensor(20.6598) 32\n",
      "Epoch[97/300] Loss: 225.090 224.483 0.607 tensor(7202.8647) tensor(19.4233) 32\n",
      "Epoch[98/300] Loss: 197.718 197.038 0.679 tensor(6326.9634) tensor(21.7363) 32\n",
      "Epoch[99/300] Loss: 203.201 202.560 0.641 tensor(6502.4370) tensor(20.5130) 32\n",
      "Epoch[100/300] Loss: 193.829 193.160 0.669 tensor(6202.5273) tensor(21.4185) 32\n",
      "Epoch[101/300] Loss: 203.159 202.543 0.615 tensor(6501.0762) tensor(19.6882) 32\n",
      "Epoch[102/300] Loss: 190.822 190.160 0.662 tensor(6106.2969) tensor(21.1685) 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[103/300] Loss: 183.627 182.916 0.712 tensor(5876.0771) tensor(22.7719) 32\n",
      "Epoch[104/300] Loss: 187.488 186.792 0.696 tensor(5999.6147) tensor(22.2804) 32\n",
      "Epoch[105/300] Loss: 178.180 177.470 0.710 tensor(5701.7578) tensor(22.7056) 32\n",
      "Epoch[106/300] Loss: 182.034 181.335 0.699 tensor(5825.0820) tensor(22.3561) 32\n",
      "Epoch[107/300] Loss: 147.550 146.848 0.703 tensor(4721.6094) tensor(22.4879) 32\n",
      "Epoch[108/300] Loss: 154.658 154.003 0.655 tensor(4949.0430) tensor(20.9602) 32\n",
      "Epoch[109/300] Loss: 140.642 139.967 0.675 tensor(4500.5298) tensor(21.5964) 32\n",
      "Epoch[110/300] Loss: 134.457 133.789 0.668 tensor(4302.6172) tensor(21.3841) 32\n",
      "Epoch[111/300] Loss: 142.210 141.537 0.673 tensor(4550.7227) tensor(21.5408) 32\n",
      "Epoch[112/300] Loss: 131.756 131.112 0.645 tensor(4216.1982) tensor(20.6248) 32\n",
      "Epoch[113/300] Loss: 116.349 115.688 0.661 tensor(3723.1792) tensor(21.1558) 32\n",
      "Epoch[114/300] Loss: 110.779 110.120 0.659 tensor(3544.9326) tensor(21.0856) 32\n",
      "Epoch[115/300] Loss: 113.132 112.458 0.673 tensor(3620.2131) tensor(21.5478) 32\n",
      "Epoch[116/300] Loss: 117.852 117.187 0.665 tensor(3771.2563) tensor(21.2802) 32\n",
      "Epoch[117/300] Loss: 124.769 124.098 0.672 tensor(3992.6208) tensor(21.4919) 32\n",
      "Epoch[118/300] Loss: 118.254 117.572 0.682 tensor(3784.1204) tensor(21.8163) 32\n",
      "Epoch[119/300] Loss: 195.311 194.576 0.735 tensor(6249.9463) tensor(23.5115) 32\n",
      "Epoch[120/300] Loss: 263.684 262.950 0.734 tensor(8437.9033) tensor(23.5015) 32\n",
      "Epoch[121/300] Loss: 176.447 175.611 0.836 tensor(5646.2954) tensor(26.7428) 32\n",
      "Epoch[122/300] Loss: 144.423 143.652 0.771 tensor(4621.5342) tensor(24.6802) 32\n",
      "Epoch[123/300] Loss: 118.553 117.763 0.790 tensor(3793.7065) tensor(25.2807) 32\n",
      "Epoch[124/300] Loss: 98.792 98.033 0.760 tensor(3161.3599) tensor(24.3125) 32\n",
      "Epoch[125/300] Loss: 104.929 104.138 0.791 tensor(3357.7258) tensor(25.3007) 32\n",
      "Epoch[126/300] Loss: 99.553 98.823 0.731 tensor(3185.7034) tensor(23.3833) 32\n",
      "Epoch[127/300] Loss: 86.799 86.036 0.762 tensor(2777.5623) tensor(24.3947) 32\n",
      "Epoch[128/300] Loss: 88.917 88.182 0.736 tensor(2845.3582) tensor(23.5407) 32\n",
      "Epoch[129/300] Loss: 75.309 74.585 0.724 tensor(2409.8811) tensor(23.1712) 32\n",
      "Epoch[130/300] Loss: 79.931 79.143 0.789 tensor(2557.8079) tensor(25.2452) 32\n",
      "Epoch[131/300] Loss: 79.956 79.252 0.704 tensor(2558.6060) tensor(22.5372) 32\n",
      "Epoch[132/300] Loss: 72.291 71.581 0.711 tensor(2313.3237) tensor(22.7449) 32\n",
      "Epoch[133/300] Loss: 66.340 65.641 0.699 tensor(2122.8809) tensor(22.3676) 32\n",
      "Epoch[134/300] Loss: 67.553 66.819 0.734 tensor(2161.7004) tensor(23.4946) 32\n",
      "Epoch[135/300] Loss: 67.148 66.420 0.728 tensor(2148.7336) tensor(23.2987) 32\n",
      "Epoch[136/300] Loss: 69.977 69.290 0.688 tensor(2239.2764) tensor(22.0000) 32\n",
      "Epoch[137/300] Loss: 65.141 64.409 0.732 tensor(2084.5161) tensor(23.4213) 32\n",
      "Epoch[138/300] Loss: 59.818 59.110 0.709 tensor(1914.1873) tensor(22.6776) 32\n",
      "Epoch[139/300] Loss: 67.076 66.370 0.706 tensor(2146.4443) tensor(22.5927) 32\n",
      "Epoch[140/300] Loss: 74.939 74.209 0.731 tensor(2398.0537) tensor(23.3771) 32\n",
      "Epoch[141/300] Loss: 107.576 106.851 0.725 tensor(3442.4409) tensor(23.1967) 32\n",
      "Epoch[142/300] Loss: 125.552 124.795 0.757 tensor(4017.6597) tensor(24.2123) 32\n",
      "Epoch[143/300] Loss: 121.401 120.610 0.791 tensor(3884.8396) tensor(25.3049) 32\n",
      "Epoch[144/300] Loss: 121.733 120.926 0.807 tensor(3895.4709) tensor(25.8371) 32\n",
      "Epoch[145/300] Loss: 83.044 82.246 0.798 tensor(2657.4097) tensor(25.5354) 32\n",
      "Epoch[146/300] Loss: 72.770 71.961 0.809 tensor(2328.6538) tensor(25.8954) 32\n",
      "Epoch[147/300] Loss: 63.895 63.102 0.793 tensor(2044.6409) tensor(25.3625) 32\n",
      "Epoch[148/300] Loss: 64.182 63.417 0.765 tensor(2053.8396) tensor(24.4927) 32\n",
      "Epoch[149/300] Loss: 52.581 51.839 0.742 tensor(1682.6027) tensor(23.7592) 32\n",
      "Epoch[150/300] Loss: 47.964 47.184 0.780 tensor(1534.8409) tensor(24.9662) 32\n",
      "Epoch[151/300] Loss: 44.864 44.109 0.755 tensor(1435.6324) tensor(24.1496) 32\n",
      "Epoch[152/300] Loss: 41.530 40.774 0.756 tensor(1328.9509) tensor(24.1859) 32\n",
      "Epoch[153/300] Loss: 51.230 50.472 0.758 tensor(1639.3674) tensor(24.2691) 32\n",
      "Epoch[154/300] Loss: 46.803 46.088 0.716 tensor(1497.7112) tensor(22.9084) 32\n",
      "Epoch[155/300] Loss: 38.292 37.525 0.768 tensor(1225.3500) tensor(24.5621) 32\n",
      "Epoch[156/300] Loss: 48.670 47.928 0.742 tensor(1557.4438) tensor(23.7367) 32\n",
      "Epoch[157/300] Loss: 42.123 41.385 0.739 tensor(1347.9410) tensor(23.6343) 32\n",
      "Epoch[158/300] Loss: 41.690 40.937 0.753 tensor(1334.0801) tensor(24.0997) 32\n",
      "Epoch[159/300] Loss: 41.399 40.630 0.770 tensor(1324.7762) tensor(24.6314) 32\n",
      "Epoch[160/300] Loss: 53.280 52.513 0.767 tensor(1704.9586) tensor(24.5392) 32\n",
      "Epoch[161/300] Loss: 79.393 78.627 0.766 tensor(2540.5833) tensor(24.5058) 32\n",
      "Epoch[162/300] Loss: 100.856 100.023 0.833 tensor(3227.3911) tensor(26.6679) 32\n",
      "Epoch[163/300] Loss: 92.630 91.793 0.837 tensor(2964.1580) tensor(26.7825) 32\n",
      "Epoch[164/300] Loss: 68.890 68.052 0.838 tensor(2204.4907) tensor(26.8150) 32\n",
      "Epoch[165/300] Loss: 50.302 49.436 0.866 tensor(1609.6768) tensor(27.7218) 32\n",
      "Epoch[166/300] Loss: 38.253 37.436 0.817 tensor(1224.0942) tensor(26.1513) 32\n",
      "Epoch[167/300] Loss: 43.714 42.885 0.829 tensor(1398.8578) tensor(26.5348) 32\n",
      "Epoch[168/300] Loss: 36.404 35.583 0.821 tensor(1164.9237) tensor(26.2708) 32\n",
      "Epoch[169/300] Loss: 36.554 35.763 0.792 tensor(1169.7426) tensor(25.3294) 32\n",
      "Epoch[170/300] Loss: 28.793 28.021 0.772 tensor(921.3749) tensor(24.7023) 32\n",
      "Epoch[171/300] Loss: 29.522 28.764 0.757 tensor(944.6887) tensor(24.2332) 32\n",
      "Epoch[172/300] Loss: 27.483 26.707 0.776 tensor(879.4625) tensor(24.8387) 32\n",
      "Epoch[173/300] Loss: 27.085 26.346 0.739 tensor(866.7141) tensor(23.6508) 32\n",
      "Epoch[174/300] Loss: 26.515 25.750 0.764 tensor(848.4663) tensor(24.4504) 32\n",
      "Epoch[175/300] Loss: 27.019 26.244 0.775 tensor(864.5984) tensor(24.7947) 32\n",
      "Epoch[176/300] Loss: 24.236 23.493 0.742 tensor(775.5383) tensor(23.7571) 32\n",
      "Epoch[177/300] Loss: 23.976 23.234 0.742 tensor(767.2344) tensor(23.7462) 32\n",
      "Epoch[178/300] Loss: 19.553 18.790 0.763 tensor(625.7047) tensor(24.4216) 32\n",
      "Epoch[179/300] Loss: 21.672 20.944 0.728 tensor(693.5098) tensor(23.3022) 32\n",
      "Epoch[180/300] Loss: 22.623 21.876 0.747 tensor(723.9401) tensor(23.9007) 32\n",
      "Epoch[181/300] Loss: 29.448 28.726 0.723 tensor(942.3486) tensor(23.1303) 32\n",
      "Epoch[182/300] Loss: 28.814 28.113 0.701 tensor(922.0384) tensor(22.4195) 32\n",
      "Epoch[183/300] Loss: 146.859 146.061 0.798 tensor(4699.4980) tensor(25.5322) 32\n",
      "Epoch[184/300] Loss: 124.409 123.555 0.854 tensor(3981.0867) tensor(27.3196) 32\n",
      "Epoch[185/300] Loss: 99.739 98.827 0.912 tensor(3191.6394) tensor(29.1850) 32\n",
      "Epoch[186/300] Loss: 50.851 49.954 0.897 tensor(1627.2272) tensor(28.6935) 32\n",
      "Epoch[187/300] Loss: 42.074 41.138 0.936 tensor(1346.3832) tensor(29.9528) 32\n",
      "Epoch[188/300] Loss: 27.617 26.766 0.851 tensor(883.7291) tensor(27.2316) 32\n",
      "Epoch[189/300] Loss: 24.017 23.182 0.835 tensor(768.5455) tensor(26.7268) 32\n",
      "Epoch[190/300] Loss: 19.896 19.083 0.812 tensor(636.6572) tensor(25.9893) 32\n",
      "Epoch[191/300] Loss: 20.166 19.310 0.856 tensor(645.3209) tensor(27.4051) 32\n",
      "Epoch[192/300] Loss: 18.448 17.616 0.832 tensor(590.3308) tensor(26.6329) 32\n",
      "Epoch[193/300] Loss: 24.743 23.935 0.808 tensor(791.7695) tensor(25.8437) 32\n",
      "Epoch[194/300] Loss: 17.519 16.707 0.812 tensor(560.6011) tensor(25.9734) 32\n",
      "Epoch[195/300] Loss: 16.700 15.893 0.807 tensor(534.4070) tensor(25.8361) 32\n",
      "Epoch[196/300] Loss: 19.733 18.940 0.793 tensor(631.4401) tensor(25.3676) 32\n",
      "Epoch[197/300] Loss: 17.218 16.423 0.795 tensor(550.9896) tensor(25.4539) 32\n",
      "Epoch[198/300] Loss: 13.811 12.981 0.829 tensor(441.9364) tensor(26.5370) 32\n",
      "Epoch[199/300] Loss: 15.709 14.920 0.789 tensor(502.6984) tensor(25.2624) 32\n",
      "Epoch[200/300] Loss: 15.606 14.890 0.716 tensor(499.3957) tensor(22.9009) 32\n",
      "Epoch[201/300] Loss: 20.855 20.116 0.739 tensor(667.3571) tensor(23.6591) 32\n",
      "Epoch[202/300] Loss: 18.946 18.212 0.734 tensor(606.2761) tensor(23.4935) 32\n",
      "Epoch[203/300] Loss: 16.765 16.033 0.732 tensor(536.4756) tensor(23.4132) 32\n",
      "Epoch[204/300] Loss: 42.615 41.836 0.780 tensor(1363.6949) tensor(24.9453) 32\n",
      "Epoch[205/300] Loss: 102.799 101.982 0.817 tensor(3289.5706) tensor(26.1437) 32\n",
      "Epoch[206/300] Loss: 82.715 81.863 0.852 tensor(2646.8655) tensor(27.2537) 32\n",
      "Epoch[207/300] Loss: 66.071 65.216 0.855 tensor(2114.2683) tensor(27.3522) 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[208/300] Loss: 37.177 36.322 0.855 tensor(1189.6725) tensor(27.3724) 32\n",
      "Epoch[209/300] Loss: 24.181 23.327 0.854 tensor(773.7858) tensor(27.3321) 32\n",
      "Epoch[210/300] Loss: 22.702 21.879 0.823 tensor(726.4648) tensor(26.3213) 32\n",
      "Epoch[211/300] Loss: 16.815 15.984 0.831 tensor(538.0646) tensor(26.5926) 32\n",
      "Epoch[212/300] Loss: 13.643 12.771 0.871 tensor(436.5737) tensor(27.8873) 32\n",
      "Epoch[213/300] Loss: 11.911 11.121 0.790 tensor(381.1391) tensor(25.2795) 32\n",
      "Epoch[214/300] Loss: 12.817 12.036 0.782 tensor(410.1533) tensor(25.0139) 32\n",
      "Epoch[215/300] Loss: 13.351 12.563 0.788 tensor(427.2310) tensor(25.2112) 32\n",
      "Epoch[216/300] Loss: 14.795 13.976 0.820 tensor(473.4495) tensor(26.2315) 32\n",
      "Epoch[217/300] Loss: 11.744 10.982 0.762 tensor(375.8019) tensor(24.3904) 32\n",
      "Epoch[218/300] Loss: 10.834 10.037 0.797 tensor(346.6972) tensor(25.5191) 32\n",
      "Epoch[219/300] Loss: 12.398 11.644 0.755 tensor(396.7481) tensor(24.1530) 32\n",
      "Epoch[220/300] Loss: 15.813 15.067 0.746 tensor(506.0295) tensor(23.8727) 32\n",
      "Epoch[221/300] Loss: 14.140 13.366 0.775 tensor(452.4902) tensor(24.7866) 32\n",
      "Epoch[222/300] Loss: 10.383 9.640 0.743 tensor(332.2713) tensor(23.7762) 32\n",
      "Epoch[223/300] Loss: 10.485 9.709 0.776 tensor(335.5164) tensor(24.8236) 32\n",
      "Epoch[224/300] Loss: 12.738 11.994 0.744 tensor(407.6263) tensor(23.8136) 32\n",
      "Epoch[225/300] Loss: 23.920 23.187 0.732 tensor(765.4255) tensor(23.4317) 32\n",
      "Epoch[226/300] Loss: 22.020 21.274 0.747 tensor(704.6470) tensor(23.8898) 32\n",
      "Epoch[227/300] Loss: 31.930 31.197 0.733 tensor(1021.7469) tensor(23.4542) 32\n",
      "Epoch[228/300] Loss: 88.787 88.011 0.777 tensor(2841.1919) tensor(24.8555) 32\n",
      "Epoch[229/300] Loss: 89.059 88.209 0.850 tensor(2849.8931) tensor(27.1941) 32\n",
      "Epoch[230/300] Loss: 48.350 47.510 0.839 tensor(1547.1949) tensor(26.8634) 32\n",
      "Epoch[231/300] Loss: 30.848 29.991 0.857 tensor(987.1351) tensor(27.4346) 32\n",
      "Epoch[232/300] Loss: 19.113 18.241 0.872 tensor(611.6285) tensor(27.9102) 32\n",
      "Epoch[233/300] Loss: 14.728 13.928 0.800 tensor(471.2804) tensor(25.5900) 32\n",
      "Epoch[234/300] Loss: 11.710 10.895 0.816 tensor(374.7359) tensor(26.1059) 32\n",
      "Epoch[235/300] Loss: 11.714 10.904 0.810 tensor(374.8329) tensor(25.9072) 32\n",
      "Epoch[236/300] Loss: 10.103 9.277 0.826 tensor(323.2989) tensor(26.4238) 32\n",
      "Epoch[237/300] Loss: 21.139 20.383 0.756 tensor(676.4519) tensor(24.2077) 32\n",
      "Epoch[238/300] Loss: 8.334 7.555 0.779 tensor(266.6852) tensor(24.9365) 32\n",
      "Epoch[239/300] Loss: 11.121 10.351 0.771 tensor(355.8752) tensor(24.6567) 32\n",
      "Epoch[240/300] Loss: 7.870 7.113 0.757 tensor(251.8301) tensor(24.2196) 32\n",
      "Epoch[241/300] Loss: 13.695 12.966 0.728 tensor(438.2302) tensor(23.3036) 32\n",
      "Epoch[242/300] Loss: 7.853 7.076 0.776 tensor(251.2848) tensor(24.8423) 32\n",
      "Epoch[243/300] Loss: 8.953 8.237 0.716 tensor(286.4944) tensor(22.9064) 32\n",
      "Epoch[244/300] Loss: 11.009 10.280 0.728 tensor(352.2835) tensor(23.3102) 32\n",
      "Epoch[245/300] Loss: 8.940 8.147 0.793 tensor(286.0919) tensor(25.3831) 32\n",
      "Epoch[246/300] Loss: 9.608 8.922 0.686 tensor(307.4570) tensor(21.9558) 32\n",
      "Epoch[247/300] Loss: 11.516 10.814 0.702 tensor(368.5085) tensor(22.4685) 32\n",
      "Epoch[248/300] Loss: 38.758 37.989 0.769 tensor(1240.2590) tensor(24.6183) 32\n",
      "Epoch[249/300] Loss: 79.733 78.915 0.818 tensor(2551.4438) tensor(26.1700) 32\n",
      "Epoch[250/300] Loss: 67.320 66.442 0.878 tensor(2154.2532) tensor(28.1092) 32\n",
      "Epoch[251/300] Loss: 33.983 33.152 0.832 tensor(1087.4629) tensor(26.6097) 32\n",
      "Epoch[252/300] Loss: 22.568 21.774 0.794 tensor(722.1680) tensor(25.4154) 32\n",
      "Epoch[253/300] Loss: 16.505 15.668 0.837 tensor(528.1752) tensor(26.7914) 32\n",
      "Epoch[254/300] Loss: 11.519 10.712 0.807 tensor(368.6064) tensor(25.8255) 32\n",
      "Epoch[255/300] Loss: 6.606 5.854 0.752 tensor(211.4018) tensor(24.0582) 32\n",
      "Epoch[256/300] Loss: 8.702 7.889 0.813 tensor(278.4764) tensor(26.0181) 32\n",
      "Epoch[257/300] Loss: 10.712 9.948 0.764 tensor(342.7917) tensor(24.4440) 32\n",
      "Epoch[258/300] Loss: 7.960 7.161 0.799 tensor(254.7309) tensor(25.5666) 32\n",
      "Epoch[259/300] Loss: 5.387 4.635 0.753 tensor(172.3923) tensor(24.0861) 32\n",
      "Epoch[260/300] Loss: 6.175 5.415 0.759 tensor(197.5869) tensor(24.2980) 32\n",
      "Epoch[261/300] Loss: 5.937 5.194 0.743 tensor(189.9966) tensor(23.7843) 32\n",
      "Epoch[262/300] Loss: 5.937 5.212 0.725 tensor(189.9814) tensor(23.1895) 32\n",
      "Epoch[263/300] Loss: 6.851 6.141 0.710 tensor(219.2311) tensor(22.7131) 32\n",
      "Epoch[264/300] Loss: 4.540 3.843 0.697 tensor(145.2841) tensor(22.3193) 32\n",
      "Epoch[265/300] Loss: 4.049 3.320 0.729 tensor(129.5758) tensor(23.3349) 32\n",
      "Epoch[266/300] Loss: 4.657 3.957 0.700 tensor(149.0125) tensor(22.3892) 32\n",
      "Epoch[267/300] Loss: 6.325 5.632 0.692 tensor(202.3896) tensor(22.1539) 32\n",
      "Epoch[268/300] Loss: 6.683 5.985 0.698 tensor(213.8602) tensor(22.3418) 32\n",
      "Epoch[269/300] Loss: 6.990 6.286 0.704 tensor(223.6811) tensor(22.5244) 32\n",
      "Epoch[270/300] Loss: 9.225 8.516 0.709 tensor(295.1927) tensor(22.6764) 32\n",
      "Epoch[271/300] Loss: 11.753 11.042 0.711 tensor(376.0949) tensor(22.7517) 32\n",
      "Epoch[272/300] Loss: 25.865 25.115 0.749 tensor(827.6652) tensor(23.9773) 32\n",
      "Epoch[273/300] Loss: 106.329 105.560 0.769 tensor(3402.5376) tensor(24.6042) 32\n",
      "Epoch[274/300] Loss: 77.036 76.184 0.852 tensor(2465.1418) tensor(27.2533) 32\n",
      "Epoch[275/300] Loss: 49.344 48.534 0.810 tensor(1579.0222) tensor(25.9249) 32\n",
      "Epoch[276/300] Loss: 25.315 24.496 0.819 tensor(810.0818) tensor(26.2144) 32\n",
      "Epoch[277/300] Loss: 16.825 16.022 0.803 tensor(538.4026) tensor(25.7106) 32\n",
      "Epoch[278/300] Loss: 8.388 7.561 0.827 tensor(268.4043) tensor(26.4679) 32\n",
      "Epoch[279/300] Loss: 11.274 10.496 0.778 tensor(360.7544) tensor(24.8922) 32\n",
      "Epoch[280/300] Loss: 6.853 6.068 0.785 tensor(219.2964) tensor(25.1245) 32\n",
      "Epoch[281/300] Loss: 10.207 9.400 0.807 tensor(326.6244) tensor(25.8223) 32\n",
      "Epoch[282/300] Loss: 5.892 5.122 0.770 tensor(188.5425) tensor(24.6371) 32\n",
      "Epoch[283/300] Loss: 5.976 5.233 0.744 tensor(191.2432) tensor(23.7955) 32\n",
      "Epoch[284/300] Loss: 7.027 6.339 0.687 tensor(224.8594) tensor(21.9979) 32\n",
      "Epoch[285/300] Loss: 5.130 4.381 0.749 tensor(164.1640) tensor(23.9612) 32\n",
      "Epoch[286/300] Loss: 4.665 3.925 0.740 tensor(149.2708) tensor(23.6763) 32\n",
      "Epoch[287/300] Loss: 10.611 9.879 0.733 tensor(339.5659) tensor(23.4537) 32\n",
      "Epoch[288/300] Loss: 4.270 3.546 0.725 tensor(136.6529) tensor(23.1881) 32\n",
      "Epoch[289/300] Loss: 5.327 4.548 0.780 tensor(170.4799) tensor(24.9460) 32\n",
      "Epoch[290/300] Loss: 6.381 5.643 0.739 tensor(204.2064) tensor(23.6384) 32\n",
      "Epoch[291/300] Loss: 11.631 10.858 0.772 tensor(372.1763) tensor(24.7064) 32\n",
      "Epoch[292/300] Loss: 31.278 30.528 0.750 tensor(1000.8962) tensor(23.9893) 32\n",
      "Epoch[293/300] Loss: 32.637 31.864 0.773 tensor(1044.3796) tensor(24.7208) 32\n",
      "Epoch[294/300] Loss: 26.771 26.006 0.765 tensor(856.6700) tensor(24.4835) 32\n",
      "Epoch[295/300] Loss: 26.028 25.240 0.788 tensor(832.9023) tensor(25.2272) 32\n",
      "Epoch[296/300] Loss: 16.916 16.165 0.751 tensor(541.3253) tensor(24.0415) 32\n",
      "Epoch[297/300] Loss: 11.675 10.864 0.811 tensor(373.6072) tensor(25.9453) 32\n",
      "Epoch[298/300] Loss: 6.309 5.545 0.764 tensor(201.8914) tensor(24.4587) 32\n",
      "Epoch[299/300] Loss: 5.815 5.018 0.797 tensor(186.0740) tensor(25.5138) 32\n",
      "Epoch[300/300] Loss: 5.343 4.585 0.758 tensor(170.9862) tensor(24.2551) 32\n",
      "Notifing Epoch[300/300] Loss: 5.343 4.585 0.758 Just because I have to put something. <Response [400]>\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for idx, (images, _) in enumerate(dataloader):\n",
    "        recon_images, mu, logvar = vae(images)\n",
    "#         print(images.shape)\n",
    "        \n",
    "        comimg = torch.cat([images * 256.0, recon_images * 256.0])\n",
    "        sample_filename = 'tmp/sample_comp_image.png'\n",
    "        save_image(comimg.data.cpu(), sample_filename)\n",
    "        \n",
    "#         display(Image(sample_filename, width=300, unconfined=True))\n",
    "        \n",
    "        loss, bce, kld = loss_fn(recon_images, images, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         to_print = \"Epoch[{}/{}] Loss: {:.3f} {:.3f} {:.3f}\".format(epoch+1, \n",
    "#                                 epochs, loss.data[0]/bs, bce.data[0]/bs, kld.data[0]/bs)\n",
    "        to_print = \"Epoch[{}/{}] Loss: {:.3f} {:.3f} {:.3f}\".format(epoch+1, \n",
    "                                epochs, loss.data/bs, bce.data/bs, kld.data/bs)\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        torch.save(vae.state_dict(), 'models/intermediate/cvae.{}-imgs_{}-epch_{}-{}'.format(model_name, len(dataset.imgs), epoch, epochs))\n",
    "        \n",
    "    print(to_print, loss.data, kld.data, bs)\n",
    "\n",
    "# notify to android when finished training\n",
    "notify(to_print, priority=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'models/cvae.{}-imgs_{}-epch_{}'.format(model_name, len(dataset.imgs), epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
