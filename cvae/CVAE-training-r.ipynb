{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import PIL\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "\n",
    "from pushover import notify\n",
    "from utils import makegif\n",
    "from random import randint\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32 # batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1339, 42)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "dataset = datasets.ImageFolder(root='trainings/roll_imgs_velo_partial', transform=transforms.Compose([\n",
    "#     transforms.Resize(64),\n",
    "    transforms.ToTensor(), \n",
    "]))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "len(dataset.imgs), len(dataloader)\n",
    "# size of input = 3 x 128 x 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed input for debugging\n",
    "fixed_x, _ = next(iter(dataloader))\n",
    "save_image(fixed_x, 'outputs/real_image.png')\n",
    "\n",
    "# Image('outputs/real_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 128])\n",
      "tensor([])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[1][0].shape)\n",
    "sample = dataset[1][0]\n",
    "print(sample[sample<0])\n",
    "HSIZE = 2048 #9216 # 1024\n",
    "ZDIM =  32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "#         print(\"flatten: \", input.shape)\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=HSIZE):\n",
    "        return input.view(input.size(0), size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=HSIZE, z_dim=ZDIM):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2), # -> [32, 32, 31, 31] 63\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # -> [32, 64, 14, 14] 31\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2), # -> [32, 128, 6, 6] 14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2), # -> [32, 256, 2, 2] 6\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2), # -> Null -> [32, 512, 2, 2] \n",
    "            nn.ReLU(), \n",
    "            Flatten() # -> [32, 1024]  -> [32, 2048]\n",
    "            # [32, a, b, c] -> [32, abc]\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(), \n",
    "            nn.ConvTranspose2d(h_dim, 256 , kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             UnFlatten(),\n",
    "#             nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "#             nn.Sigmoid(),\n",
    "#         )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "#         print(\"bottle: \",mu.shape, logvar.shape)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "#         print(\"======== Encode ========\", x.shape)\n",
    "        h = self.encoder(x)\n",
    "#         print(\"enc(x): \", h.shape)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "#         print(\"z.shape: \", z.shape)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "#         print(\"======== Decode ========\", z.shape)\n",
    "        z = self.fc3(z)\n",
    "#         print(\"fc3(z).shape: \", z.shape)\n",
    "        z = self.decoder(z)\n",
    "#         print(\"decode(fc3(z)).shape: \", z.shape)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "#         print(z.shape)\n",
    "        z = self.decode(z)\n",
    "#         print(z.shape, mu.shape, logvar.shape)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channels = fixed_x.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(image_channels=image_channels).to(device)\n",
    "# model.load_state_dict(torch.load('vae.torch', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "#     BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "    BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD, BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 70\n",
    "model_name = \"rgb_bce\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/70] Loss: 639.442 639.401 0.040 tensor(20462.1367) tensor(1.2894) 32\n",
      "Epoch[2/70] Loss: 573.755 573.724 0.031 tensor(18360.1445) tensor(0.9856) 32\n",
      "Epoch[3/70] Loss: 664.159 664.138 0.021 tensor(21253.0840) tensor(0.6722) 32\n",
      "Epoch[4/70] Loss: 515.224 515.205 0.018 tensor(16487.1523) tensor(0.5803) 32\n",
      "Epoch[5/70] Loss: 517.584 517.566 0.017 tensor(16562.6777) tensor(0.5545) 32\n",
      "Epoch[6/70] Loss: 613.105 613.088 0.017 tensor(19619.3613) tensor(0.5461) 32\n",
      "Epoch[7/70] Loss: 642.225 642.209 0.016 tensor(20551.2129) tensor(0.5102) 32\n",
      "Epoch[8/70] Loss: 686.369 686.353 0.017 tensor(21963.8145) tensor(0.5319) 32\n",
      "Epoch[9/70] Loss: 651.599 651.583 0.016 tensor(20851.1719) tensor(0.5140) 32\n",
      "Epoch[10/70] Loss: 653.062 653.046 0.016 tensor(20897.9961) tensor(0.5145) 32\n",
      "Epoch[11/70] Loss: 537.476 537.464 0.013 tensor(17199.2461) tensor(0.4070) 32\n",
      "Epoch[12/70] Loss: 583.300 583.288 0.012 tensor(18665.5996) tensor(0.3697) 32\n",
      "Epoch[13/70] Loss: 523.692 523.679 0.013 tensor(16758.1289) tensor(0.4037) 32\n",
      "Epoch[14/70] Loss: 624.572 624.561 0.011 tensor(19986.3164) tensor(0.3606) 32\n",
      "Epoch[15/70] Loss: 642.517 642.506 0.011 tensor(20560.5547) tensor(0.3661) 32\n",
      "Epoch[16/70] Loss: 601.864 601.852 0.012 tensor(19259.6406) tensor(0.3764) 32\n",
      "Epoch[17/70] Loss: 587.178 587.169 0.009 tensor(18789.6875) tensor(0.2770) 32\n",
      "Epoch[18/70] Loss: 522.988 522.976 0.012 tensor(16735.6094) tensor(0.3804) 32\n",
      "Epoch[19/70] Loss: 551.959 551.948 0.011 tensor(17662.6855) tensor(0.3399) 32\n",
      "Epoch[20/70] Loss: 612.754 612.738 0.015 tensor(19608.1133) tensor(0.4937) 32\n",
      "Epoch[21/70] Loss: 613.157 613.147 0.010 tensor(19621.0312) tensor(0.3267) 32\n",
      "Epoch[22/70] Loss: 613.053 613.041 0.011 tensor(19617.6816) tensor(0.3628) 32\n",
      "Epoch[23/70] Loss: 582.896 582.886 0.010 tensor(18652.6738) tensor(0.3216) 32\n",
      "Epoch[24/70] Loss: 670.649 670.641 0.008 tensor(21460.7617) tensor(0.2658) 32\n",
      "Epoch[25/70] Loss: 602.241 602.232 0.009 tensor(19271.6973) tensor(0.2819) 32\n",
      "Epoch[26/70] Loss: 560.807 560.796 0.011 tensor(17945.8242) tensor(0.3443) 32\n",
      "Epoch[27/70] Loss: 559.036 559.021 0.015 tensor(17889.1504) tensor(0.4864) 32\n",
      "Epoch[28/70] Loss: 630.705 630.695 0.010 tensor(20182.5469) tensor(0.3107) 32\n",
      "Epoch[29/70] Loss: 583.709 583.700 0.008 tensor(18678.6738) tensor(0.2617) 32\n",
      "Epoch[30/70] Loss: 733.624 733.615 0.009 tensor(23475.9590) tensor(0.2941) 32\n",
      "Epoch[31/70] Loss: 587.057 587.049 0.007 tensor(18785.8105) tensor(0.2348) 32\n",
      "Epoch[32/70] Loss: 685.076 685.068 0.008 tensor(21922.4160) tensor(0.2422) 32\n",
      "Epoch[33/70] Loss: 671.507 671.500 0.008 tensor(21488.2363) tensor(0.2403) 32\n",
      "Epoch[34/70] Loss: 675.828 675.819 0.009 tensor(21626.5078) tensor(0.2859) 32\n",
      "Epoch[35/70] Loss: 648.221 648.214 0.008 tensor(20743.0781) tensor(0.2471) 32\n",
      "Epoch[36/70] Loss: 657.815 657.807 0.007 tensor(21050.0645) tensor(0.2251) 32\n",
      "Epoch[37/70] Loss: 595.790 595.781 0.008 tensor(19065.2695) tensor(0.2705) 32\n",
      "Epoch[38/70] Loss: 707.158 707.152 0.006 tensor(22629.0488) tensor(0.1998) 32\n",
      "Epoch[39/70] Loss: 610.043 610.036 0.007 tensor(19521.3672) tensor(0.2231) 32\n",
      "Epoch[40/70] Loss: 624.764 624.755 0.008 tensor(19992.4375) tensor(0.2643) 32\n",
      "Epoch[41/70] Loss: 601.779 601.771 0.008 tensor(19256.9219) tensor(0.2450) 32\n",
      "Epoch[42/70] Loss: 624.383 624.377 0.006 tensor(19980.2520) tensor(0.2004) 32\n",
      "Epoch[43/70] Loss: 616.646 616.641 0.006 tensor(19732.6875) tensor(0.1911) 32\n",
      "Epoch[44/70] Loss: 601.044 601.032 0.013 tensor(19233.4238) tensor(0.4105) 32\n",
      "Epoch[45/70] Loss: 693.155 693.147 0.008 tensor(22180.9473) tensor(0.2548) 32\n",
      "Epoch[46/70] Loss: 565.050 565.032 0.018 tensor(18081.5898) tensor(0.5724) 32\n",
      "Epoch[47/70] Loss: 647.755 647.750 0.006 tensor(20728.1641) tensor(0.1798) 32\n",
      "Epoch[48/70] Loss: 644.224 644.208 0.017 tensor(20615.1797) tensor(0.5364) 32\n",
      "Epoch[49/70] Loss: 591.806 591.799 0.007 tensor(18937.7852) tensor(0.2167) 32\n",
      "Epoch[50/70] Loss: 655.609 655.602 0.007 tensor(20979.4844) tensor(0.2230) 32\n",
      "Epoch[51/70] Loss: 662.452 662.446 0.006 tensor(21198.4648) tensor(0.2063) 32\n",
      "Epoch[52/70] Loss: 612.911 612.904 0.007 tensor(19613.1582) tensor(0.2169) 32\n",
      "Epoch[53/70] Loss: 602.013 602.006 0.007 tensor(19264.4141) tensor(0.2244) 32\n",
      "Epoch[54/70] Loss: 671.862 671.856 0.006 tensor(21499.5859) tensor(0.1898) 32\n",
      "Epoch[55/70] Loss: 553.677 553.670 0.007 tensor(17717.6602) tensor(0.2113) 32\n",
      "Epoch[56/70] Loss: 613.317 613.311 0.006 tensor(19626.1543) tensor(0.1896) 32\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for idx, (images, _) in enumerate(dataloader):\n",
    "        recon_images, mu, logvar = vae(images)\n",
    "#         print(images)\n",
    "        loss, bce, kld = loss_fn(recon_images, images, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#         to_print = \"Epoch[{}/{}] Loss: {:.3f} {:.3f} {:.3f}\".format(epoch+1, \n",
    "#                                 epochs, loss.data[0]/bs, bce.data[0]/bs, kld.data[0]/bs)\n",
    "        to_print = \"Epoch[{}/{}] Loss: {:.3f} {:.3f} {:.3f}\".format(epoch+1, \n",
    "                                epochs, loss.data/bs, bce.data/bs, kld.data/bs)\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(vae.state_dict(), 'models/cvae.{}-imgs_{}-epch_{}-{}'.format(model_name, len(dataset.imgs), epoch, epochs))\n",
    "    print(to_print, loss.data, kld.data, bs)\n",
    "\n",
    "# notify to android when finished training\n",
    "notify(to_print, priority=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'models/cvae.{}-imgs_{}-epch_{}'.format(model_name, len(dataset.imgs), epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
